# pokAI

A self-play deep reinforcement learning system for heads-up No-Limit Texas Hold‚Äôem (HUNL), implementing a Trinal-Clip PPO loss over Siamese-style policy/value networks, with Elo-based opponent selection and a small fixed population of static strategies for diversity.

---

## Overview

pokAI trains a two-player (‚Äúheads-up‚Äù) No-Limit Hold‚Äôem poker agent via on-policy Proximal Policy Optimization with:

- **Trinal-Clip** policy loss (per Alpha Hold‚Äôem paper)  
- **Clipped value loss**  
- **Siamese-style** neural network shared across policy & value  
- **Elo**-based opponent selection from a small pool of historical selves  
- Three static baseline opponents (always-fold, always-all-in, random legal) to maintain policy diversity  

It simulates hands in-memory (no external poker server), collects Slumbot-style replays, builds experiences per street, and updates the policy & value networks via PPO.

---

## Features

- üöÄ **Self-Play Loop** with population-based opponent selection  
- üìä **Elo Rating** to measure and drive improvement  
- ‚ôüÔ∏è **Static Strategies** (fold-only, all-in, random) for exploration  
- üìà **TensorBoard** logging for live metrics (policy loss, value loss, mBB/hand, Elo)  
- üîÑ **Experience Replay Builder** parses action histories into training examples  
- üÉè **Fast, In-Memory** poker simulator compatible with Slumbot pipeline  

---

## Getting Started

### Prerequisites

- **Python 3.8+**  
- **PyTorch** (tested on 1.13+) with CUDA or MPS support  
- **NumPy**, **Matplotlib**, **TensorBoard**  
- (Optional) GPU recommended for faster training  

### Installation

1. Clone the repository  
   ```bash
   git clone https://github.com/acaen007/pokAI.git
   cd pokAI





Configuration
Key hyperparameters in the main loop:

| Variable            | Description                                   | Default    |
| ------------------- | --------------------------------------------- | ---------- |
| `num_cycles`        | Number of self-play + train cycles            | `100`      |
| `match_hands`       | Hands per match for Elo evaluation            | `5000`     |
| `matches_per_cycle` | Matches per cycle for Elo update              | `1`        |
| `BIG_BLIND`         | Big blind size (chips)                        | `100`      |
| `SMALL_BLIND`       | Small blind size (chips)                      | `50`       |
| `K`                 | Top-K historical agents to sample             | `3`        |
| PPO Clip (Œµ)        | Policy clip parameter                         | `0.1`      |
| Value Clip (Œ¥‚ÇÇ, Œ¥‚ÇÉ) | From Trinal-Clip PPO (via `deltas` from hand) | per-street |
| Trinal Clip (Œ¥1)    | Avoids high variance with negative advantage  | `3`        |
