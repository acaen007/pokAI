{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ppo_utils import (\n",
    "    a_gae,\n",
    "    tc_loss_function,\n",
    "    ratio,\n",
    "    r_gamma,\n",
    "    v_loss,\n",
    "    get_action_from_probs,\n",
    "    make_model_value_function\n",
    ")\n",
    "from card_representation import CardRepresentation\n",
    "from action_representation import ActionRepresentation\n",
    "from siamese_net import PseudoSiameseNet, logits_to_probs, clone_model_weights\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a hardcoded example\n",
    "def build_card_rep_for_state(state: str) -> CardRepresentation:\n",
    "    \"\"\"\n",
    "    Given a 'state' like 'Preflop', 'Flop', 'Turn', 'River', or 'Showdown',\n",
    "    build an example CardRepresentation. This is a toy demonstration:\n",
    "      - Preflop: sets hole cards only\n",
    "      - Flop: sets hole + flop\n",
    "      - Turn: sets hole + flop + turn\n",
    "      - River or Showdown: sets hole + flop + turn + river\n",
    "    \"\"\"\n",
    "    cr = CardRepresentation()\n",
    "\n",
    "    # For demonstration, we always use the same hole cards: As, Ac\n",
    "    cr.set_preflop([(12,3), (12,2)])  # rank=12 => Ace, suits=3 => clubs, 2 => diamonds\n",
    "\n",
    "    if state in ['Flop', 'Turn', 'River', 'Showdown']:\n",
    "        flop_cards = [(7,1), (3,3), (9,2)]  # e.g. 9d, 5s, Jc\n",
    "        cr.set_flop(flop_cards)\n",
    "\n",
    "    if state in ['Turn', 'River', 'Showdown']:\n",
    "        turn_card = (5,0)  # 7h\n",
    "        cr.set_turn(turn_card)\n",
    "\n",
    "    if state in ['River', 'Showdown']:\n",
    "        river_card = (11,3) # Ks\n",
    "        cr.set_river(river_card)\n",
    "\n",
    "    return cr\n",
    "\n",
    "\n",
    "def get_action(state: str, player: int):\n",
    "    \"\"\"\n",
    "    Given a 'state' like 'Preflop', 'Flop', 'Turn', 'River', or 'Showdown',\n",
    "    and a 'player' (0 or 1), return an action for that player.\n",
    "    This is a toy demonstration.\n",
    "\n",
    "    Returns a list of 4 elements:\n",
    "        - round_id: 0-3\n",
    "        - action_index_in_round: 0-5\n",
    "        - player_id: 0, 1\n",
    "        - action_idx: 0-8, depending on the action_type\n",
    "    \"\"\"\n",
    "    if state == 'Preflop':\n",
    "        if player == 0:\n",
    "            return [0, 0, 0, 6] # hero (player_id=0) bets pot\n",
    "        else:\n",
    "            return [0, 1, 1, 2] # villain (player_id=1) calls\n",
    "    elif state == 'Flop':\n",
    "        if player == 0:\n",
    "            return [1, 0, 0, 3] # hero (player_id=0) bets small\n",
    "        else:\n",
    "            return [1, 1, 1, 2] # villain (player_id=1) calls\n",
    "    elif state == 'Turn':\n",
    "        if player == 0:\n",
    "            return [2, 0, 0, 1] # hero (player_id=0) checks\n",
    "        else:\n",
    "            return [2, 1, 1, 1] # villain (player_id=1) checks\n",
    "    elif state == 'River':\n",
    "        if player == 0:\n",
    "            return [3, 0, 0, 8] # hero (player_id=0) shoves\n",
    "        else:\n",
    "            return [3, 1, 1, 2] # villain (player_id=1) calls\n",
    "\n",
    "def build_action_rep_for_state(state: str) -> ActionRepresentation:\n",
    "    \"\"\"\n",
    "    Builds an ActionRepresentation for a given 'state'.\n",
    "    We'll fill it with a minimal set of actions so far.\n",
    "    \"\"\"\n",
    "    ar = ActionRepresentation(nb=9, max_actions_per_round=6, rounds=4)\n",
    "\n",
    "    # Suppose on Preflop we have 2 actions that happened:\n",
    "    #  - hero (player_id=0) bet pot => action_idx=6\n",
    "    #  - villain (player_id=1) calls => action_idx=1\n",
    "    # We'll fill those for *all* states up to Flop, \n",
    "    # then add more as we get further into the hand.\n",
    "    # ar.add_action(0, 0, 0, 6, legal_actions=range(9))  # channel=0\n",
    "    # ar.add_action(0, 1, 1, 2, legal_actions=range(9))  # channel=1\n",
    "\n",
    "    # if state in ['Flop','Turn','River','Showdown']:\n",
    "    #     # Let's say on the Flop there's 1 action: hero bets small => action_idx=2\n",
    "    #     ar.add_action(1, 0, 0, 3, legal_actions=range(9))  # channel=6 (round_id=1, index=0)\n",
    "\n",
    "    # if state in ['Turn','River','Showdown']:\n",
    "    #     # On Turn, hero checks => action_idx=1\n",
    "    #     ar.add_action(2, 0, 0, 1, legal_actions=range(9))  # channel=12\n",
    "\n",
    "    # if state in ['River','Showdown']:\n",
    "    #     # On River, hero shoves => action_idx=8, for example\n",
    "    #     ar.add_action(3, 0, 0, 8, legal_actions=range(9))  # channel=18\n",
    "\n",
    "    states = ['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
    "    for prev_state in states:\n",
    "        if prev_state == state: # This is inside the loop because the Hero is player 0. This way on the last iteration we don't add the villain's action\n",
    "            break\n",
    "        for player in [0, 1]:\n",
    "            round_id, action_index_in_round, player_id, action_idx = get_action(prev_state, player)\n",
    "            ar.add_action(round_id, action_index_in_round, player_id, action_idx)\n",
    "\n",
    "    return ar\n",
    "\n",
    "def build_reward_for_state(state: str) -> float:\n",
    "    \"\"\"\n",
    "    Builds a reward for a given 'state'.\n",
    "    We'll use a simple reward scheme:\n",
    "      - Preflop: -20\n",
    "      - Flop: -20\n",
    "      - Turn: -80\n",
    "      - River: 0\n",
    "      - Showdown: 240\n",
    "    \"\"\"\n",
    "    rewards = {'Preflop': -20, 'Flop': -20, 'Turn': -80, 'River': 0, 'Showdown': 240} #Showdown reward gets added to the River reward, so there are equal actions taken and results and states\n",
    "    return rewards[state]\n",
    "\n",
    "\n",
    "def get_deltas(state):\n",
    "    \"\"\"\n",
    "    Return (delta1, delta2, delta3) for the given street\n",
    "    in the trinal-clip PPO approach.\n",
    "    \"\"\"\n",
    "    delta1 = 3\n",
    "    if state == 'Preflop':\n",
    "        delta2, delta3 = 20, 10\n",
    "    elif state == 'Flop':\n",
    "        delta2, delta3 = 40, 20\n",
    "    elif state == 'Turn':\n",
    "        delta2, delta3 = 120, 80\n",
    "    elif state == 'River':\n",
    "        delta2, delta3 = 120, 120\n",
    "    elif state == 'Showdown':\n",
    "        delta2, delta3 = 120, 120\n",
    "    return (delta1, delta2, delta3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def to_torch_input(card_rep: np.array, action_rep: np.array):\n",
    "    \"\"\"\n",
    "    Convert the card_rep/card_tensor and action_rep/action_tensor\n",
    "    to torch tensors of shape (1,...) for the siamese model.\n",
    "    \"\"\"\n",
    "    card_np = card_rep[np.newaxis, ...]      # (1,6,4,13)\n",
    "    action_np = action_rep[np.newaxis, ...]# (1,24,4,9)\n",
    "    card_t = torch.from_numpy(card_np).float()\n",
    "    action_t = torch.from_numpy(action_np).float()\n",
    "    return action_t, card_t\n",
    "\n",
    "# def run_one_iteration_old(iter_idx: int, old_policy_net: PseudoSiameseNet, new_policy_net: PseudoSiameseNet,\n",
    "#                       optimizer: optim.Optimizer):\n",
    "#     \"\"\"\n",
    "#     Demonstrates a single iteration (episode) with states=[Preflop,Flop,Turn,River,Showdown]\n",
    "#     and rewards=[-20, -20, -80, 0, 240].\n",
    "#     We'll do partial PPO logic: \n",
    "#       - compute advantage,\n",
    "#       - old/new policy ratio,\n",
    "#       - trinal-clip policy loss,\n",
    "#       - a toy value loss, \n",
    "#       - gradient update => new_policy_net changes.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n=== Iteration {iter_idx} ===\")\n",
    "#     states = ['Preflop','Flop','Turn','River', 'Showdown']\n",
    "#     rewards = [-20, -20, -80, 0, 240]\n",
    "\n",
    "#     # We'll track some metrics\n",
    "#     total_pol_loss = 0\n",
    "#     total_val_loss = 0\n",
    "#     steps_count = 0\n",
    "\n",
    "#     # Build a model-based value function using the *new* net \n",
    "#     # (In typical PPO, the value function is updated simultaneously with the new policy.)\n",
    "#     model_value_func = make_model_value_function(new_policy_net, build_card_rep_for_state, build_action_rep_for_state)\n",
    "\n",
    "#     for i, st in enumerate(states[:-1]):  # skip Showdown itself\n",
    "#         # 1) Compute advantage\n",
    "#         future_rewards = rewards[i:]\n",
    "#         future_states  = states[i:]\n",
    "#         advantage = a_gae(future_rewards, future_states, model_value_func, gamma=0.999, lambda_=0.99)\n",
    "\n",
    "#         # 2) Build card/action reps for this state\n",
    "#         card_rep = build_card_rep_for_state(st)\n",
    "#         action_rep = build_action_rep_for_state(st)\n",
    "#         action_t, card_t = to_torch_input(card_rep, action_rep)\n",
    "\n",
    "#         # 3) old policy => old_probs\n",
    "#         with torch.no_grad():\n",
    "#             old_logits, _ = old_policy_net(action_t, card_t)\n",
    "#             old_probs = logits_to_probs(old_logits)[0].cpu().numpy()\n",
    "\n",
    "#         # 4) new policy => new_probs + new_value\n",
    "#         new_logits, new_value = new_policy_net(action_t, card_t)\n",
    "#         new_probs_t = logits_to_probs(new_logits)[0]\n",
    "#         new_probs = new_probs_t.detach().cpu().numpy()\n",
    "\n",
    "#         # 5) sample an action from the *new* policy\n",
    "#         action_idx = np.random.choice(len(new_probs), p=new_probs)\n",
    "\n",
    "#         # 6) ratio = new_probs[action_idx]/old_probs[action_idx]\n",
    "#         ratio_val = ratio(old_probs, new_probs, action_idx)\n",
    "\n",
    "#         # 7) policy loss\n",
    "#         deltas = get_deltas(st)\n",
    "#         pol_loss_val = tc_loss_function(ratio_val, advantage, epsilon=0.2, deltas=deltas)\n",
    "\n",
    "#         # 8) value loss\n",
    "#         #    compute r_gamma from future rewards\n",
    "#         r_g = r_gamma(np.array(future_rewards), gamma=0.999)\n",
    "#         val_loss_val = v_loss(r_g, st, deltas, value_function_fn=model_value_func)\n",
    "\n",
    "#         # 9) build a toy combined loss => do a gradient update\n",
    "#         #   - incorporate pol_loss_val and val_loss_val into the PyTorch graph \n",
    "#         #   - We'll do a negative log(prob_of_action) scaled by pol_loss_val,\n",
    "#         #     plus MSE( new_value, val_loss_val ).\n",
    "#         chosen_log_prob = torch.log(new_probs_t[action_idx] + 1e-8)\n",
    "#         pol_loss_t = torch.tensor(pol_loss_val, dtype=torch.float32)\n",
    "#         val_loss_t = torch.tensor(val_loss_val, dtype=torch.float32)\n",
    "\n",
    "#         combined_loss = - pol_loss_t * chosen_log_prob + (new_value[0] - val_loss_t)**2\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         combined_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_pol_loss += pol_loss_val\n",
    "#         total_val_loss += val_loss_val\n",
    "#         steps_count += 1\n",
    "\n",
    "#         print(f\"  State={st}, ratio={ratio_val:.3f}, advantage={advantage:.2f}, action_idx={action_idx}\")\n",
    "#         print(f\"    pol_loss={pol_loss_val:.3f}, val_loss={val_loss_val:.3f}\")\n",
    "\n",
    "#     if steps_count > 0:\n",
    "#         print(f\"=> iteration {iter_idx} done. avg pol_loss={total_pol_loss/steps_count:.3f}, avg val_loss={total_val_loss/steps_count:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandResult:\n",
    "    def __init__(self, card_reps: list, action_reps: list, actions_taken:list, rewards: list, deltas: list):\n",
    "        if len(card_reps) != len(action_reps) or len(card_reps) != len(card_reps): # We need to make sure that the length of the card_reps, action_reps and rewards are the same. Action before showdown should be a None value or smth like that\n",
    "            print(len(card_reps), len(action_reps), len(rewards))\n",
    "            raise ValueError(\"card_reps, rewards and action_reps must have the same length\") \n",
    "        self.states = list((card_rep.card_tensor, action_reps.action_tensor) for card_rep, action_reps in zip(card_reps, action_reps))\n",
    "        self.rewards = rewards\n",
    "        self.rounds = [{\n",
    "            'state': self.states[i],\n",
    "            'action_taken': actions_taken[i],\n",
    "            'reward': rewards[i],\n",
    "            'deltas': deltas[i]\n",
    "        } for i in range(len(self.states))]\n",
    "\n",
    "    def new_state(self, card_rep: CardRepresentation, action_rep: ActionRepresentation, action_taken: int, reward: int, deltas: list):\n",
    "        self.states.append((card_rep, action_rep))\n",
    "        self.rewards.append(reward)\n",
    "        self.rounds.append({\n",
    "            'state': (card_rep.card_tensor, action_rep.action_tensor),\n",
    "            'action_taken': action_taken,\n",
    "            'reward': reward,\n",
    "            'deltas': deltas\n",
    "        })\n",
    "\n",
    "\n",
    "def get_hand_result(method1: bool) -> HandResult:\n",
    "    \"\"\"\n",
    "    Given a list of 'states' like ['Preflop','Flop','Turn','River','Showdown'],\n",
    "    build a numpy array of shape (5,1) for the siamese model.\n",
    "\n",
    "    I am sure both methods output the same result.\n",
    "    \"\"\"\n",
    "    states = ['Preflop','Flop','Turn','River']\n",
    "    # empty np.array with length 5\n",
    "    if (method1):\n",
    "        card_reps = [build_card_rep_for_state(state) for state in states]\n",
    "        action_reps = [build_action_rep_for_state(state) for state in states]\n",
    "        actions_taken = [get_action(state, 0)[3] for state in states]\n",
    "        rewards = [build_reward_for_state(state) for state in states]\n",
    "        deltas = [get_deltas(state) for state in states]\n",
    "        return HandResult(card_reps, action_reps, actions_taken, rewards, deltas)\n",
    "    else:\n",
    "        for state in states:\n",
    "            card_rep = build_card_rep_for_state(state)\n",
    "            action_rep = build_action_rep_for_state(state)\n",
    "            action_taken = get_action(state, 0)[3]\n",
    "            reward = build_reward_for_state(state)\n",
    "            deltas = get_deltas(state)\n",
    "            if state == 'Preflop':\n",
    "                hand_result = HandResult([card_rep], [action_rep], [action_taken], [reward], [deltas])\n",
    "            else:\n",
    "                hand_result.new_state(card_rep, action_rep, action_taken, reward, deltas)\n",
    "        return hand_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_iteration(iter_idx: int, rounds_array, old_policy_net: PseudoSiameseNet, new_policy_net: PseudoSiameseNet,\n",
    "                      optimizer: optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Demonstrates a single iteration (episode) with states=[Preflop,Flop,Turn,River,Showdown]\n",
    "    and rewards=[-20, -20, -80, 0, 240].\n",
    "    We'll do partial PPO logic: \n",
    "      - compute advantage,\n",
    "      - old/new policy ratio,\n",
    "      - trinal-clip policy loss,\n",
    "      - a toy value loss, \n",
    "      - gradient update => new_policy_net changes.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Iteration {iter_idx} ===\")\n",
    "    # states = ['Preflop','Flop','Turn','River','Showdown']\n",
    "    # rewards = [-20, -20, -80, 0, 240]\n",
    "\n",
    "    # We'll track some metrics\n",
    "    total_pol_loss = 0\n",
    "    total_val_loss = 0\n",
    "    steps_count = 0\n",
    "\n",
    "    # Build a model-based value function using the *new* net \n",
    "    # (In typical PPO, the value function is updated simultaneously with the new policy.)\n",
    "    model_value_func = make_model_value_function(new_policy_net)\n",
    "    states, rewards = zip(*[(round[\"state\"], round[\"reward\"]) for round in rounds_array])\n",
    "\n",
    "    for i, round in enumerate(rounds_array):  # skip Showdown itself\n",
    "\n",
    "        deltas = round['deltas']\n",
    "        action_taken = round['action_taken']\n",
    "        state = round['state']\n",
    "\n",
    "        # 1) Compute advantage\n",
    "        if i < len(rounds_array) - 1:\n",
    "            future_rewards = rewards[i:]\n",
    "            future_states  = states[i:]\n",
    "            advantage_t = a_gae(future_states, future_rewards, model_value_func, gamma=0.999, lambda_=0.99)\n",
    "\n",
    "        # 2) Build card/action reps for this state\n",
    "        card_tensor = state[0]\n",
    "        action_tensor = state[1]\n",
    "        action_t, card_t = to_torch_input(card_tensor, action_tensor)\n",
    "\n",
    "        # 3) old policy => old_probs\n",
    "        old_logits, _ = old_policy_net.forward(action_t, card_t) # I think dont want to use no_grad here\n",
    "        old_probs = logits_to_probs(old_logits)[0].detach().cpu().numpy()\n",
    "\n",
    "        # 4) new policy => new_probs + new_value\n",
    "        new_logits, new_value = new_policy_net.forward(action_t, card_t)\n",
    "        new_probs_t = logits_to_probs(new_logits)[0]\n",
    "        new_probs = new_probs_t.detach().cpu().numpy()\n",
    "\n",
    "        # 5) sample an action from the *new* policy. THIS IS THE ACTION_TAKEN\n",
    "        \n",
    "\n",
    "        # 6) ratio = new_probs[action_idx]/old_probs[action_idx]\n",
    "        ratio_t = ratio(old_probs, new_probs, action_taken)\n",
    "\n",
    "        # 7) policy loss\n",
    "        deltas = round['deltas']\n",
    "        pol_loss_t = tc_loss_function(ratio_t, advantage_t, epsilon=0.2, deltas=deltas)\n",
    "        pol_loss_val = pol_loss_t.item()\n",
    "\n",
    "        # 8) value loss\n",
    "        #    compute r_gamma from future rewards\n",
    "        r_g = r_gamma(np.array(future_rewards), gamma=0.999)\n",
    "        val_loss_t = v_loss(r_g, state, deltas, new_value)\n",
    "        val_loss_val = val_loss_t.item()\n",
    "\n",
    "        # 9) build a toy combined loss => do a gradient update\n",
    "        #   - incorporate pol_loss_val and val_loss_val into the PyTorch graph \n",
    "        #   - We'll do a negative log(prob_of_action) scaled by pol_loss_val,\n",
    "        #     plus MSE( new_value, val_loss_val ).\n",
    "        chosen_log_prob = torch.log(new_probs_t[action_taken] + 1e-8)\n",
    "        pol_loss_t = torch.tensor(pol_loss_val, dtype=torch.float32)\n",
    "        val_loss_t = torch.tensor(val_loss_val, dtype=torch.float32)\n",
    "\n",
    "        combined_loss = - pol_loss_t * chosen_log_prob + (new_value[0] - val_loss_t)**2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_pol_loss += pol_loss_val\n",
    "        total_val_loss += val_loss_val\n",
    "        steps_count += 1\n",
    "        print(f\"  State {i}, ratio={ratio_t.item():.3f}, advantage={advantage_t.item():.2f}, action_idx={action_taken}\")\n",
    "        print(f\"    pol_loss={pol_loss_val:.3f}, val_loss={val_loss_val:.3f}\")\n",
    "        print(f\"   old_probs={old_probs}, new_probs={new_probs}\")\n",
    "    if steps_count > 0:\n",
    "        print(f\"=> iteration {iter_idx} done. avg pol_loss={total_pol_loss/steps_count:.3f}, avg val_loss={total_val_loss/steps_count:.3f}\")\n",
    "\n",
    "\n",
    "def train_model(model, hand_results):\n",
    "    old_policy_net = PseudoSiameseNet()\n",
    "    new_policy_net = PseudoSiameseNet()\n",
    "    clone_model_weights(model, old_policy_net)\n",
    "    clone_model_weights(model, new_policy_net)\n",
    "    optimizer = optim.Adam(new_policy_net.parameters(), lr=0.001)\n",
    "    for i, hand_result in enumerate(hand_results):\n",
    "        run_one_iteration(i, hand_result.rounds, old_policy_net, new_policy_net, optimizer)\n",
    "        # Optionally, if you want to update old_policy_net to match new_policy_net after each iteration:\n",
    "        clone_model_weights(new_policy_net, old_policy_net)\n",
    "    # Return the updated new_policy_net if needed.\n",
    "    return new_policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the model plays and generates all the states representations. You get for each round:\n",
    "# - the card representations\n",
    "# - the action representations\n",
    "# - the rewards array\n",
    "\n",
    "# Then we run the PPO algorithm to update the policy and value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_result1 = get_hand_result(True)\n",
    "hand_result2 = get_hand_result(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Here I checked both methods were working equally\n",
    "key = 'state'\n",
    "for index in range(len(hand_result1.rounds)):\n",
    "    print((hand_result1.rounds[index][key][1] == hand_result2.rounds[index][key][1]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounds_array = hand_result2.rounds\n",
    "rounds_array[0][\"reward\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_result = get_hand_result(True)\n",
    "hand_results = [hand_result for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "  [1. 0. 1. 0. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 2. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]] 240\n"
     ]
    }
   ],
   "source": [
    "# Here I wanted to manually check all the actions where being added correctly\n",
    "\n",
    "id = 3\n",
    "\n",
    "print(hand_result.rounds[id][\"state\"][1], hand_result.rounds[id][\"reward\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 0 ===\n",
      "  State 0, ratio=0.966, advantage=-1.77, action_idx=6\n",
      "    pol_loss=-1.709, val_loss=400.569\n",
      "   old_probs=[0.12034971 0.11289378 0.10960522 0.10757452 0.11895547 0.1075887\n",
      " 0.11078171 0.10497434 0.10727652], new_probs=[0.10752725 0.10844526 0.1204056  0.11732838 0.10775412 0.11255424\n",
      " 0.10696795 0.10967582 0.10934138]\n",
      "  State 1, ratio=1.094, advantage=-1.19, action_idx=3\n",
      "    pol_loss=-1.300, val_loss=1604.177\n",
      "   old_probs=[0.12046541 0.11281162 0.10955175 0.10730614 0.11871742 0.10787983\n",
      " 0.11108684 0.10496014 0.10722081], new_probs=[0.10722628 0.10717817 0.12024306 0.11734135 0.10883825 0.11348834\n",
      " 0.106752   0.10990152 0.10903112]\n",
      "  State 2, ratio=0.941, advantage=-0.80, action_idx=1\n",
      "    pol_loss=-0.753, val_loss=6414.206\n",
      "   old_probs=[0.12034764 0.11287798 0.10949554 0.10733338 0.11878126 0.10798245\n",
      " 0.11097647 0.10499205 0.10721324], new_probs=[0.10691146 0.10623211 0.12010626 0.11681299 0.11013546 0.11435253\n",
      " 0.10639744 0.11017168 0.10888004]\n",
      "  State 3, ratio=1.019, advantage=-0.80, action_idx=8\n",
      "    pol_loss=-0.816, val_loss=6420.074\n",
      "   old_probs=[0.1204685  0.11293538 0.10951557 0.10739943 0.11874954 0.1079584\n",
      " 0.11095092 0.10490146 0.10712076], new_probs=[0.10678545 0.10479902 0.12053472 0.11659136 0.11126533 0.11463814\n",
      " 0.1058623  0.1103183  0.10920545]\n",
      "=> iteration 0 done. avg pol_loss=-1.145, avg val_loss=3709.757\n"
     ]
    }
   ],
   "source": [
    "old_policy = PseudoSiameseNet()\n",
    "new_policy = PseudoSiameseNet()\n",
    "run_one_iteration(0, hand_result.rounds, old_policy, new_policy, optim.Adam(new_policy.parameters(), lr=0.001))\n",
    "\n",
    "# THERES SOMETHING WRONG WITH THE SIAMESE ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 0 ===\n",
      "  State 0, ratio=1.000, advantage=-1.77, action_idx=6\n",
      "    pol_loss=-1.770, val_loss=400.604\n",
      "   old_probs=[0.10840059 0.10487235 0.10628538 0.11452141 0.10376357 0.11191905\n",
      " 0.11618525 0.1191173  0.11493514], new_probs=[0.10840059 0.10487235 0.10628538 0.11452141 0.10376357 0.11191905\n",
      " 0.11618525 0.1191173  0.11493514]\n",
      "  State 1, ratio=1.009, advantage=-1.19, action_idx=3\n",
      "    pol_loss=-1.200, val_loss=1604.547\n",
      "   old_probs=[0.10862861 0.10486101 0.10629661 0.11436199 0.10392162 0.11185011\n",
      " 0.11631168 0.11906023 0.11470824], new_probs=[0.10800943 0.10427322 0.10619068 0.11536682 0.10440952 0.11197127\n",
      " 0.11573382 0.11955587 0.11448931]\n",
      "  State 2, ratio=0.990, advantage=-0.80, action_idx=1\n",
      "    pol_loss=-0.792, val_loss=6416.100\n",
      "   old_probs=[0.10871647 0.10480455 0.10622469 0.11424129 0.10398738 0.11173145\n",
      " 0.11635831 0.11929496 0.11464089], new_probs=[0.10747356 0.10374237 0.10582772 0.11520442 0.10539279 0.11239198\n",
      " 0.11530242 0.1205129  0.11415179]\n",
      "  State 3, ratio=0.992, advantage=-0.80, action_idx=8\n",
      "    pol_loss=-0.794, val_loss=6423.455\n",
      "   old_probs=[0.10878094 0.1046961  0.10637003 0.11411642 0.1039954  0.11184186\n",
      " 0.11621099 0.11940306 0.11458515], new_probs=[0.10720339 0.10235551 0.10534256 0.1151471  0.10634746 0.11312596\n",
      " 0.11539838 0.12139202 0.11368752]\n",
      "=> iteration 0 done. avg pol_loss=-1.139, avg val_loss=3711.177\n",
      "\n",
      "=== Iteration 1 ===\n",
      "  State 0, ratio=1.000, advantage=-1.77, action_idx=6\n",
      "    pol_loss=-1.769, val_loss=406.444\n",
      "   old_probs=[0.10706502 0.10178564 0.10574824 0.11504294 0.10642291 0.11340437\n",
      " 0.11523038 0.12158639 0.11371406], new_probs=[0.10706502 0.10178564 0.10574824 0.11504294 0.10642291 0.11340437\n",
      " 0.11523038 0.12158639 0.11371406]\n",
      "  State 1, ratio=1.000, advantage=-1.19, action_idx=3\n",
      "    pol_loss=-1.189, val_loss=1618.932\n",
      "   old_probs=[0.1068944  0.10132885 0.10511699 0.11508681 0.10712866 0.11390039\n",
      " 0.11569539 0.12204298 0.1128054 ], new_probs=[0.10653687 0.10013876 0.10461158 0.11509236 0.10822038 0.11493454\n",
      " 0.11561614 0.1229876  0.11186181]\n",
      "  State 2, ratio=0.969, advantage=-0.80, action_idx=1\n",
      "    pol_loss=-0.775, val_loss=6451.023\n",
      "   old_probs=[0.10683627 0.10109802 0.10480388 0.11506525 0.10746557 0.11390346\n",
      " 0.11580551 0.1224253  0.11259676], new_probs=[0.10583364 0.09799591 0.10348566 0.1147312  0.11018167 0.11651829\n",
      " 0.11570311 0.124937   0.11061347]\n",
      "  State 3, ratio=0.972, advantage=-0.80, action_idx=8\n",
      "    pol_loss=-0.778, val_loss=6469.615\n",
      "   old_probs=[0.10672253 0.10085291 0.10468716 0.11510447 0.10762982 0.11407765\n",
      " 0.11585241 0.12265791 0.1124152 ], new_probs=[0.10442815 0.09427098 0.10225108 0.11427528 0.11241265 0.11900782\n",
      " 0.11603827 0.1280168  0.10929909]\n",
      "=> iteration 1 done. avg pol_loss=-1.128, avg val_loss=3736.503\n",
      "\n",
      "=== Iteration 2 ===\n",
      "  State 0, ratio=1.000, advantage=-1.76, action_idx=6\n",
      "    pol_loss=-1.765, val_loss=413.883\n",
      "   old_probs=[0.10616639 0.09624764 0.10482982 0.11421348 0.11066416 0.11757623\n",
      " 0.11432876 0.12582788 0.11014563], new_probs=[0.10616639 0.09624764 0.10482982 0.11421348 0.11066416 0.11757623\n",
      " 0.11432876 0.12582788 0.11014563]\n",
      "  State 1, ratio=0.994, advantage=-1.19, action_idx=3\n",
      "    pol_loss=-1.180, val_loss=1648.873\n",
      "   old_probs=[0.10440344 0.09266298 0.10252825 0.11384451 0.11333859 0.12011374\n",
      " 0.11594088 0.12903826 0.10812938], new_probs=[0.10335512 0.08897205 0.10165268 0.11317151 0.11553087 0.12298876\n",
      " 0.11557023 0.1325768  0.10618192]\n",
      "  State 2, ratio=0.890, advantage=-0.80, action_idx=1\n",
      "    pol_loss=-0.712, val_loss=6545.446\n",
      "   old_probs=[0.10348665 0.09105464 0.10149141 0.11358867 0.11435636 0.1211452\n",
      " 0.11644602 0.13078023 0.10765077], new_probs=[0.10046156 0.08106736 0.09887946 0.11087552 0.12062421 0.12989299\n",
      " 0.11495689 0.14126287 0.10197906]\n",
      "  State 3, ratio=0.894, advantage=-0.80, action_idx=8\n",
      "    pol_loss=-0.714, val_loss=6608.552\n",
      "   old_probs=[0.10296071 0.09004653 0.10094523 0.11360133 0.11465398 0.12191623\n",
      " 0.11677943 0.13185284 0.10724369], new_probs=[0.09755234 0.07030462 0.09567541 0.10760959 0.12687723 0.13992381\n",
      " 0.11265267 0.15354179 0.09586246]\n",
      "=> iteration 2 done. avg pol_loss=-1.093, avg val_loss=3804.188\n",
      "\n",
      "=== Iteration 3 ===\n",
      "  State 0, ratio=1.000, advantage=-1.75, action_idx=6\n",
      "    pol_loss=-1.746, val_loss=429.392\n",
      "   old_probs=[0.10491543 0.0838449  0.10377422 0.11028853 0.11835178 0.12740852\n",
      " 0.11106091 0.13766839 0.10268725], new_probs=[0.10491543 0.0838449  0.10377422 0.11028853 0.11835178 0.12740852\n",
      " 0.11106091 0.13766839 0.10268725]\n",
      "  State 1, ratio=0.957, advantage=-1.18, action_idx=3\n",
      "    pol_loss=-1.129, val_loss=1737.689\n",
      "   old_probs=[0.09960303 0.06883615 0.09732597 0.10589694 0.12814285 0.14214565\n",
      " 0.11074632 0.15412992 0.09317315], new_probs=[0.09814414 0.05937708 0.09545407 0.1013625  0.13497317 0.15376592\n",
      " 0.10525779 0.16652347 0.08514179]\n",
      "  State 2, ratio=0.627, advantage=-0.80, action_idx=1\n",
      "    pol_loss=-0.638, val_loss=6837.139\n",
      "   old_probs=[0.0967687  0.06247494 0.09416807 0.10386556 0.13226265 0.1490236\n",
      " 0.1095784  0.16279155 0.08906654], new_probs=[0.09078632 0.03919261 0.08747523 0.08621789 0.15104824 0.18465848\n",
      " 0.09221756 0.20142522 0.06697849]\n",
      "  State 3, ratio=0.542, advantage=-0.80, action_idx=8\n",
      "    pol_loss=-0.638, val_loss=7038.334\n",
      "   old_probs=[0.09497268 0.05910124 0.09247286 0.10285766 0.1342422  0.15308301\n",
      " 0.10876025 0.16779357 0.0867165 ], new_probs=[0.08089208 0.02169959 0.07720992 0.06634305 0.1662682  0.22352155\n",
      " 0.07290675 0.24415502 0.04700387]\n",
      "=> iteration 3 done. avg pol_loss=-1.038, avg val_loss=4010.639\n",
      "\n",
      "=== Iteration 4 ===\n",
      "  State 0, ratio=1.000, advantage=-1.69, action_idx=6\n",
      "    pol_loss=-1.685, val_loss=467.445\n",
      "   old_probs=[0.10473182 0.05646696 0.1029341  0.09406332 0.13729946 0.15796474\n",
      " 0.09595402 0.16975628 0.08082931], new_probs=[0.10473182 0.05646696 0.1029341  0.09406332 0.13729946 0.15796474\n",
      " 0.09595402 0.16975628 0.08082931]\n",
      "  State 1, ratio=0.738, advantage=-1.16, action_idx=3\n",
      "    pol_loss=-0.926, val_loss=1995.880\n",
      "   old_probs=[0.08677516 0.02249091 0.08301374 0.06435789 0.1666358  0.22165029\n",
      " 0.07123927 0.23789987 0.04593712], new_probs=[0.07874469 0.01272369 0.07503013 0.04751402 0.17701247 0.2545877\n",
      " 0.05198388 0.27189022 0.03051317]\n",
      "  State 2, ratio=0.156, advantage=-0.79, action_idx=1\n",
      "    pol_loss=-0.633, val_loss=7677.836\n",
      "   old_probs=[0.0759607  0.01398357 0.07213701 0.05199627 0.17455089 0.24864289\n",
      " 0.05900616 0.2696582  0.03406432], new_probs=[0.04982705 0.0021876  0.04686549 0.01700045 0.18128581 0.32567406\n",
      " 0.01965124 0.3488899  0.00861843]\n",
      "  State 3, ratio=0.056, advantage=-0.79, action_idx=8\n",
      "    pol_loss=-0.633, val_loss=8274.828\n",
      "   old_probs=[0.06986448 0.01072455 0.06631103 0.046099   0.17699857 0.26244178\n",
      " 0.05270256 0.28616145 0.02869651], new_probs=[2.6787873e-02 2.2142900e-04 2.4823489e-02 4.0768357e-03 1.6144970e-01\n",
      " 3.7601167e-01 4.9685044e-03 4.0005228e-01 1.6083230e-03]\n",
      "=> iteration 4 done. avg pol_loss=-0.969, avg val_loss=4603.997\n",
      "\n",
      "=== Iteration 5 ===\n",
      "  State 0, ratio=1.000, advantage=-1.52, action_idx=6\n",
      "    pol_loss=-1.519, val_loss=563.047\n",
      "   old_probs=[0.09841371 0.01695565 0.09588742 0.04747483 0.17392823 0.23475184\n",
      " 0.0500363  0.24819247 0.03435959], new_probs=[0.09841371 0.01695565 0.09588742 0.04747483 0.17392823 0.23475184\n",
      " 0.0500363  0.24819247 0.03435959]\n",
      "  State 1, ratio=0.282, advantage=-1.10, action_idx=3\n",
      "    pol_loss=-0.877, val_loss=2728.827\n",
      "   old_probs=[0.03726408 0.00039264 0.03420065 0.00511545 0.17223829 0.36466992\n",
      " 0.00623007 0.37767714 0.00221173], new_probs=[2.4339499e-02 6.0746974e-05 2.1743599e-02 1.4414546e-03 1.5590793e-01\n",
      " 3.9246809e-01 1.7025175e-03 4.0183824e-01 4.9795699e-04]\n",
      "  State 2, ratio=0.003, advantage=-0.77, action_idx=1\n",
      "    pol_loss=-0.620, val_loss=10021.401\n",
      "   old_probs=[2.0451942e-02 5.6690835e-05 1.8413790e-02 1.5625474e-03 1.5021800e-01\n",
      " 3.9457461e-01 1.9944455e-03 4.1219991e-01 5.2800815e-04], new_probs=[5.20957541e-03 1.76374598e-07 4.26621875e-03 2.61011282e-05\n",
      " 1.00472614e-01 4.40740019e-01 3.37820056e-05 4.49246436e-01\n",
      " 5.04619538e-06]\n",
      "  State 3, ratio=0.000, advantage=-0.77, action_idx=8\n",
      "    pol_loss=-0.620, val_loss=11791.949\n",
      "   old_probs=[1.45861525e-02 1.98838370e-05 1.30406907e-02 8.21661844e-04\n",
      " 1.37241215e-01 4.06208098e-01 1.06739334e-03 4.26772505e-01\n",
      " 2.42332244e-04], new_probs=[9.6178660e-04 2.0679616e-10 7.0328242e-04 2.2604604e-07 5.9457511e-02\n",
      " 4.6739402e-01 3.2206827e-07 4.7148275e-01 2.4797490e-08]\n",
      "=> iteration 5 done. avg pol_loss=-0.909, avg val_loss=6276.306\n",
      "\n",
      "=== Iteration 6 ===\n",
      "  State 0, ratio=1.000, advantage=-1.11, action_idx=6\n",
      "    pol_loss=-1.105, val_loss=804.639\n",
      "   old_probs=[0.05701163 0.00051789 0.05173516 0.00408639 0.18486495 0.34126154\n",
      " 0.00451516 0.35388726 0.00212006], new_probs=[0.05701163 0.00051789 0.05173516 0.00408639 0.18486495 0.34126154\n",
      " 0.00451516 0.35388726 0.00212006]\n",
      "  State 1, ratio=0.028, advantage=-0.94, action_idx=3\n",
      "    pol_loss=-0.756, val_loss=4865.659\n",
      "   old_probs=[2.5882679e-03 3.3387129e-09 1.8932398e-03 1.1300108e-06 8.0920562e-02\n",
      " 4.6007639e-01 1.6133657e-06 4.5451865e-01 1.7104695e-07], new_probs=[8.9723180e-04 2.8198645e-11 5.9181423e-04 3.1895453e-08 5.8421586e-02\n",
      " 4.7620618e-01 4.2526601e-08 4.6388304e-01 3.0708607e-09]\n",
      "  State 2, ratio=0.000, advantage=-0.73, action_idx=1\n",
      "    pol_loss=-0.588, val_loss=16626.248\n",
      "   old_probs=[5.0445105e-04 8.4231780e-12 3.3679948e-04 1.8557309e-08 4.8443750e-02\n",
      " 4.7789839e-01 2.8755721e-08 4.7281653e-01 1.5248435e-09], new_probs=[1.6949563e-05 3.2551061e-18 8.1940225e-06 2.2005176e-13 1.6625019e-02\n",
      " 5.0246203e-01 3.4921813e-13 4.8088777e-01 5.8633865e-15]\n",
      "  State 3, ratio=0.000, advantage=-0.73, action_idx=8\n",
      "    pol_loss=-0.588, val_loss=22085.988\n",
      "   old_probs=[2.0628217e-04 3.3436416e-13 1.3187186e-04 2.0351902e-09 3.6295298e-02\n",
      " 4.8339397e-01 3.2437748e-09 4.7997260e-01 1.1956960e-10], new_probs=[2.9127878e-07 1.5522523e-25 9.8981523e-08 5.2078490e-19 4.5176060e-03\n",
      " 5.1249266e-01 9.9551817e-19 4.8298931e-01 4.3048567e-21]\n",
      "=> iteration 6 done. avg pol_loss=-0.759, avg val_loss=11095.634\n",
      "\n",
      "=== Iteration 7 ===\n",
      "  State 0, ratio=1.000, advantage=-0.18, action_idx=6\n",
      "    pol_loss=-0.179, val_loss=1431.283\n",
      "   old_probs=[1.0433945e-02 1.4527862e-07 7.7608330e-03 6.5615991e-06 1.2421899e-01\n",
      " 4.2569417e-01 7.9196689e-06 4.3187532e-01 2.0342382e-06], new_probs=[1.0433945e-02 1.4527862e-07 7.7608330e-03 6.5615991e-06 1.2421899e-01\n",
      " 4.2569417e-01 7.9196689e-06 4.3187532e-01 2.0342382e-06]\n",
      "  State 1, ratio=0.000, advantage=-0.61, action_idx=3\n",
      "    pol_loss=-0.487, val_loss=11407.551\n",
      "   old_probs=[4.9327264e-06 3.8728641e-21 1.8995349e-06 4.6720945e-16 1.1526033e-02\n",
      " 5.1266938e-01 8.8224871e-16 4.7579771e-01 1.2257773e-17], new_probs=[5.0445453e-07 1.7031236e-25 1.5114728e-07 1.3614677e-19 5.7409904e-03\n",
      " 5.1980853e-01 2.5531806e-19 4.7444978e-01 2.1722914e-21]\n",
      "  State 2, ratio=0.000, advantage=-0.65, action_idx=1\n",
      "    pol_loss=-0.518, val_loss=35924.871\n",
      "   old_probs=[9.5902138e-08 5.4126778e-28 2.7162580e-08 3.5860727e-21 3.2225081e-03\n",
      " 5.1864243e-01 7.8997695e-21 4.7813493e-01 2.6875330e-23], new_probs=[8.1980575e-11 1.8037514e-41 1.0656687e-11 4.1531182e-32 3.6252858e-04\n",
      " 5.3038430e-01 8.7375189e-32 4.6925315e-01 6.6940408e-35]\n",
      "  State 3, ratio=0.000, advantage=-0.65, action_idx=8\n",
      "    pol_loss=-0.518, val_loss=53552.602\n",
      "   old_probs=[1.1087574e-08 1.0053842e-31 2.6791172e-09 5.9624576e-24 1.5939493e-03\n",
      " 5.1889765e-01 1.3947949e-23 4.7950846e-01 2.2383387e-26], new_probs=[1.8266794e-14 0.0000000e+00 9.9670790e-16 1.1210388e-44 2.5321897e-05\n",
      " 5.3572631e-01 2.2420775e-44 4.6424836e-01 0.0000000e+00]\n",
      "=> iteration 7 done. avg pol_loss=-0.425, avg val_loss=25579.077\n",
      "\n",
      "=== Iteration 8 ===\n",
      "  State 0, ratio=0.185, advantage=1.72, action_idx=6\n",
      "    pol_loss=1.377, val_loss=3122.113\n",
      "   old_probs=[3.3379404e-04 2.0636567e-14 1.6208323e-04 1.5983392e-11 4.5940209e-02\n",
      " 4.8048446e-01 1.8494118e-11 4.7307944e-01 2.4017251e-12], new_probs=[3.3379404e-04 2.0636567e-14 1.6208323e-04 1.5983392e-11 4.5940209e-02\n",
      " 4.8048446e-01 1.8494118e-11 4.7307944e-01 2.4017251e-12]\n",
      "  State 1, ratio=0.000, advantage=0.07, action_idx=3\n",
      "    pol_loss=0.060, val_loss=32291.502\n",
      "   old_probs=[2.1022449e-11 5.6051939e-45 1.8998783e-12 1.4093833e-35 2.4661687e-04\n",
      " 5.3978264e-01 2.3154687e-35 4.5997074e-01 2.7109077e-38], new_probs=[3.4784453e-13 0.0000000e+00 1.8904252e-14 2.8544450e-42 6.9091453e-05\n",
      " 5.4504377e-01 4.2739603e-42 4.5488718e-01 2.8025969e-45]\n",
      "  State 2, ratio=0.000, advantage=-0.47, action_idx=1\n",
      "    pol_loss=-0.376, val_loss=94780.516\n",
      "   old_probs=[4.1480245e-15 0.0000000e+00 1.6632085e-16 0.0000000e+00 1.6316393e-05\n",
      " 5.4287714e-01 0.0000000e+00 4.5710653e-01 0.0000000e+00], new_probs=[1.1434376e-20 0.0000000e+00 9.2292914e-23 0.0000000e+00 2.8366364e-07\n",
      " 5.6879741e-01 0.0000000e+00 4.3120229e-01 0.0000000e+00]\n",
      "  State 3, ratio=0.000, advantage=-0.47, action_idx=8\n",
      "    pol_loss=-0.376, val_loss=153560.734\n",
      "   old_probs=[3.7752202e-17 0.0000000e+00 9.8507626e-19 0.0000000e+00 3.6410302e-06\n",
      " 5.4135764e-01 0.0000000e+00 4.5863876e-01 0.0000000e+00], new_probs=[1.2289801e-27 0.0000000e+00 1.5198857e-30 0.0000000e+00 1.6214922e-09\n",
      " 6.0055083e-01 0.0000000e+00 3.9944923e-01 0.0000000e+00]\n",
      "=> iteration 8 done. avg pol_loss=0.171, avg val_loss=70938.716\n",
      "\n",
      "=== Iteration 9 ===\n",
      "  State 0, ratio=0.000, advantage=5.39, action_idx=6\n",
      "    pol_loss=4.311, val_loss=7835.632\n",
      "   old_probs=[8.4562896e-07 2.1653392e-26 1.8481535e-07 1.6439362e-21 6.9036265e-03\n",
      " 5.2456403e-01 1.7290260e-21 4.6853128e-01 7.9526160e-23], new_probs=[8.4562896e-07 2.1653392e-26 1.8481535e-07 1.6439362e-21 6.9036265e-03\n",
      " 5.2456403e-01 1.7290260e-21 4.6853128e-01 7.9526160e-23]\n",
      "  State 1, ratio=0.000, advantage=1.39, action_idx=3\n",
      "    pol_loss=1.113, val_loss=99765.797\n",
      "   old_probs=[5.9141297e-21 0.0000000e+00 3.0876742e-23 0.0000000e+00 2.0623666e-07\n",
      " 6.1535859e-01 0.0000000e+00 3.8464111e-01 0.0000000e+00], new_probs=[5.1639296e-24 0.0000000e+00 1.0413320e-26 0.0000000e+00 2.0562341e-08\n",
      " 6.4673918e-01 0.0000000e+00 3.5326087e-01 0.0000000e+00]\n",
      "  State 2, ratio=0.000, advantage=-0.13, action_idx=1\n",
      "    pol_loss=-0.103, val_loss=278549.812\n",
      "   old_probs=[2.7122236e-28 0.0000000e+00 2.1984761e-31 0.0000000e+00 9.2350438e-10\n",
      " 6.3644141e-01 0.0000000e+00 3.6355862e-01 0.0000000e+00], new_probs=[6.4878718e-38 0.0000000e+00 2.6036125e-42 0.0000000e+00 6.0588687e-13\n",
      " 7.2766107e-01 0.0000000e+00 2.7233890e-01 0.0000000e+00]\n",
      "  State 3, ratio=0.000, advantage=-0.13, action_idx=8\n",
      "    pol_loss=-0.103, val_loss=474206.406\n",
      "   old_probs=[2.2279494e-32 0.0000000e+00 6.5610959e-36 0.0000000e+00 4.5244364e-11\n",
      " 6.4321727e-01 0.0000000e+00 3.5678276e-01 0.0000000e+00], new_probs=[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.9091550e-17\n",
      " 8.0360764e-01 0.0000000e+00 1.9639237e-01 0.0000000e+00]\n",
      "=> iteration 9 done. avg pol_loss=1.304, avg val_loss=215089.412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PseudoSiameseNet(\n",
       "  (action_conv): Sequential(\n",
       "    (0): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (card_conv): Sequential(\n",
       "    (0): Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (action_fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (card_fc): Sequential(\n",
       "    (0): Linear(in_features=192, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fusion_fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (policy_head): Linear(in_features=256, out_features=9, bias=True)\n",
       "  (value_head): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot = PseudoSiameseNet()\n",
    "train_model(bot, hand_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 4, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_result.rounds[0][\"state\"][1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
