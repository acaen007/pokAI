{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ppo_utils import (\n",
    "    a_gae,\n",
    "    tc_loss_function,\n",
    "    ratio,\n",
    "    r_gamma,\n",
    "    v_loss,\n",
    "    get_action_from_probs,\n",
    "    make_model_value_function\n",
    ")\n",
    "from card_representation import CardRepresentation\n",
    "from action_representation import ActionRepresentation\n",
    "from siamese_net import PseudoSiameseNet, logits_to_probs, clone_model_weights\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a hardcoded example\n",
    "def build_card_rep_for_state(state: str) -> CardRepresentation:\n",
    "    \"\"\"\n",
    "    Given a 'state' like 'Preflop', 'Flop', 'Turn', 'River', or 'Showdown',\n",
    "    build an example CardRepresentation. This is a toy demonstration:\n",
    "      - Preflop: sets hole cards only\n",
    "      - Flop: sets hole + flop\n",
    "      - Turn: sets hole + flop + turn\n",
    "      - River or Showdown: sets hole + flop + turn + river\n",
    "    \"\"\"\n",
    "    cr = CardRepresentation()\n",
    "\n",
    "    # For demonstration, we always use the same hole cards: As, Ac\n",
    "    cr.set_preflop([(12,3), (12,2)])  # rank=12 => Ace, suits=3 => clubs, 2 => diamonds\n",
    "\n",
    "    if state in ['Flop', 'Turn', 'River', 'Showdown']:\n",
    "        flop_cards = [(7,1), (3,3), (9,2)]  # e.g. 9d, 5s, Jc\n",
    "        cr.set_flop(flop_cards)\n",
    "\n",
    "    if state in ['Turn', 'River', 'Showdown']:\n",
    "        turn_card = (5,0)  # 7h\n",
    "        cr.set_turn(turn_card)\n",
    "\n",
    "    if state in ['River', 'Showdown']:\n",
    "        river_card = (11,3) # Ks\n",
    "        cr.set_river(river_card)\n",
    "\n",
    "    return cr\n",
    "\n",
    "\n",
    "def get_action(state: str, player: int):\n",
    "    \"\"\"\n",
    "    Given a 'state' like 'Preflop', 'Flop', 'Turn', 'River', or 'Showdown',\n",
    "    and a 'player' (0 or 1), return an action for that player.\n",
    "    This is a toy demonstration.\n",
    "\n",
    "    Returns a list of 4 elements:\n",
    "        - round_id: 0-3\n",
    "        - action_index_in_round: 0-5\n",
    "        - player_id: 0, 1\n",
    "        - action_idx: 0-8, depending on the action_type\n",
    "    \"\"\"\n",
    "    if state == 'Preflop':\n",
    "        if player == 0:\n",
    "            return [0, 0, 0, 6] # hero (player_id=0) bets pot\n",
    "        else:\n",
    "            return [0, 1, 1, 2] # villain (player_id=1) calls\n",
    "    elif state == 'Flop':\n",
    "        if player == 0:\n",
    "            return [1, 0, 0, 3] # hero (player_id=0) bets small\n",
    "        else:\n",
    "            return [1, 1, 1, 2] # villain (player_id=1) calls\n",
    "    elif state == 'Turn':\n",
    "        if player == 0:\n",
    "            return [2, 0, 0, 1] # hero (player_id=0) checks\n",
    "        else:\n",
    "            return [2, 1, 1, 1] # villain (player_id=1) checks\n",
    "    elif state == 'River':\n",
    "        if player == 0:\n",
    "            return [3, 0, 0, 8] # hero (player_id=0) shoves\n",
    "        else:\n",
    "            return [3, 1, 1, 2] # villain (player_id=1) calls\n",
    "\n",
    "def build_action_rep_for_state(state: str) -> ActionRepresentation:\n",
    "    \"\"\"\n",
    "    Builds an ActionRepresentation for a given 'state'.\n",
    "    We'll fill it with a minimal set of actions so far.\n",
    "    \"\"\"\n",
    "    ar = ActionRepresentation(nb=9, max_actions_per_round=6, rounds=4)\n",
    "\n",
    "    # Suppose on Preflop we have 2 actions that happened:\n",
    "    #  - hero (player_id=0) bet pot => action_idx=6\n",
    "    #  - villain (player_id=1) calls => action_idx=1\n",
    "    # We'll fill those for *all* states up to Flop, \n",
    "    # then add more as we get further into the hand.\n",
    "    # ar.add_action(0, 0, 0, 6, legal_actions=range(9))  # channel=0\n",
    "    # ar.add_action(0, 1, 1, 2, legal_actions=range(9))  # channel=1\n",
    "\n",
    "    # if state in ['Flop','Turn','River','Showdown']:\n",
    "    #     # Let's say on the Flop there's 1 action: hero bets small => action_idx=2\n",
    "    #     ar.add_action(1, 0, 0, 3, legal_actions=range(9))  # channel=6 (round_id=1, index=0)\n",
    "\n",
    "    # if state in ['Turn','River','Showdown']:\n",
    "    #     # On Turn, hero checks => action_idx=1\n",
    "    #     ar.add_action(2, 0, 0, 1, legal_actions=range(9))  # channel=12\n",
    "\n",
    "    # if state in ['River','Showdown']:\n",
    "    #     # On River, hero shoves => action_idx=8, for example\n",
    "    #     ar.add_action(3, 0, 0, 8, legal_actions=range(9))  # channel=18\n",
    "\n",
    "    states = ['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
    "    for prev_state in states:\n",
    "        if prev_state == state: # This is inside the loop because the Hero is player 0. This way on the last iteration we don't add the villain's action\n",
    "            break\n",
    "        for player in [0, 1]:\n",
    "            round_id, action_index_in_round, player_id, action_idx = get_action(prev_state, player)\n",
    "            ar.add_action(round_id, action_index_in_round, player_id, action_idx)\n",
    "\n",
    "    return ar\n",
    "\n",
    "def build_reward_for_state(state: str) -> float:\n",
    "    \"\"\"\n",
    "    Builds a reward for a given 'state'.\n",
    "    We'll use a simple reward scheme:\n",
    "      - Preflop: -20\n",
    "      - Flop: -20\n",
    "      - Turn: -80\n",
    "      - River: 0\n",
    "      - Showdown: 240\n",
    "    \"\"\"\n",
    "    rewards = {'Preflop': -20, 'Flop': -20, 'Turn': -80, 'River': 0, 'Showdown': 240} #Showdown reward gets added to the River reward, so there are equal actions taken and results and states\n",
    "    return rewards[state]\n",
    "\n",
    "\n",
    "def get_deltas(state):\n",
    "    \"\"\"\n",
    "    Return (delta1, delta2, delta3) for the given street\n",
    "    in the trinal-clip PPO approach.\n",
    "    \"\"\"\n",
    "    delta1 = 3\n",
    "    if state == 'Preflop':\n",
    "        delta2, delta3 = 20, 10\n",
    "    elif state == 'Flop':\n",
    "        delta2, delta3 = 40, 20\n",
    "    elif state == 'Turn':\n",
    "        delta2, delta3 = 120, 80\n",
    "    elif state == 'River':\n",
    "        delta2, delta3 = 120, 120\n",
    "    elif state == 'Showdown':\n",
    "        delta2, delta3 = 120, 120\n",
    "    return (delta1, delta2, delta3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def to_torch_input(card_rep: np.array, action_rep: np.array):\n",
    "    \"\"\"\n",
    "    Convert the card_rep/card_tensor and action_rep/action_tensor\n",
    "    to torch tensors of shape (1,...) for the siamese model.\n",
    "    \"\"\"\n",
    "    card_np = card_rep[np.newaxis, ...]      # (1,6,4,13)\n",
    "    action_np = action_rep[np.newaxis, ...]# (1,24,4,9)\n",
    "    card_t = torch.from_numpy(card_np).float()\n",
    "    action_t = torch.from_numpy(action_np).float()\n",
    "    return action_t, card_t\n",
    "\n",
    "# def run_one_iteration_old(iter_idx: int, old_policy_net: PseudoSiameseNet, new_policy_net: PseudoSiameseNet,\n",
    "#                       optimizer: optim.Optimizer):\n",
    "#     \"\"\"\n",
    "#     Demonstrates a single iteration (episode) with states=[Preflop,Flop,Turn,River,Showdown]\n",
    "#     and rewards=[-20, -20, -80, 0, 240].\n",
    "#     We'll do partial PPO logic: \n",
    "#       - compute advantage,\n",
    "#       - old/new policy ratio,\n",
    "#       - trinal-clip policy loss,\n",
    "#       - a toy value loss, \n",
    "#       - gradient update => new_policy_net changes.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\n=== Iteration {iter_idx} ===\")\n",
    "#     states = ['Preflop','Flop','Turn','River', 'Showdown']\n",
    "#     rewards = [-20, -20, -80, 0, 240]\n",
    "\n",
    "#     # We'll track some metrics\n",
    "#     total_pol_loss = 0\n",
    "#     total_val_loss = 0\n",
    "#     steps_count = 0\n",
    "\n",
    "#     # Build a model-based value function using the *new* net \n",
    "#     # (In typical PPO, the value function is updated simultaneously with the new policy.)\n",
    "#     model_value_func = make_model_value_function(new_policy_net, build_card_rep_for_state, build_action_rep_for_state)\n",
    "\n",
    "#     for i, st in enumerate(states[:-1]):  # skip Showdown itself\n",
    "#         # 1) Compute advantage\n",
    "#         future_rewards = rewards[i:]\n",
    "#         future_states  = states[i:]\n",
    "#         advantage = a_gae(future_rewards, future_states, model_value_func, gamma=0.999, lambda_=0.99)\n",
    "\n",
    "#         # 2) Build card/action reps for this state\n",
    "#         card_rep = build_card_rep_for_state(st)\n",
    "#         action_rep = build_action_rep_for_state(st)\n",
    "#         action_t, card_t = to_torch_input(card_rep, action_rep)\n",
    "\n",
    "#         # 3) old policy => old_probs\n",
    "#         with torch.no_grad():\n",
    "#             old_logits, _ = old_policy_net(action_t, card_t)\n",
    "#             old_probs = logits_to_probs(old_logits)[0].cpu().numpy()\n",
    "\n",
    "#         # 4) new policy => new_probs + new_value\n",
    "#         new_logits, new_value = new_policy_net(action_t, card_t)\n",
    "#         new_probs_t = logits_to_probs(new_logits)[0]\n",
    "#         new_probs = new_probs_t.detach().cpu().numpy()\n",
    "\n",
    "#         # 5) sample an action from the *new* policy\n",
    "#         action_idx = np.random.choice(len(new_probs), p=new_probs)\n",
    "\n",
    "#         # 6) ratio = new_probs[action_idx]/old_probs[action_idx]\n",
    "#         ratio_val = ratio(old_probs, new_probs, action_idx)\n",
    "\n",
    "#         # 7) policy loss\n",
    "#         deltas = get_deltas(st)\n",
    "#         pol_loss_val = tc_loss_function(ratio_val, advantage, epsilon=0.2, deltas=deltas)\n",
    "\n",
    "#         # 8) value loss\n",
    "#         #    compute r_gamma from future rewards\n",
    "#         r_g = r_gamma(np.array(future_rewards), gamma=0.999)\n",
    "#         val_loss_val = v_loss(r_g, st, deltas, value_function_fn=model_value_func)\n",
    "\n",
    "#         # 9) build a toy combined loss => do a gradient update\n",
    "#         #   - incorporate pol_loss_val and val_loss_val into the PyTorch graph \n",
    "#         #   - We'll do a negative log(prob_of_action) scaled by pol_loss_val,\n",
    "#         #     plus MSE( new_value, val_loss_val ).\n",
    "#         chosen_log_prob = torch.log(new_probs_t[action_idx] + 1e-8)\n",
    "#         pol_loss_t = torch.tensor(pol_loss_val, dtype=torch.float32)\n",
    "#         val_loss_t = torch.tensor(val_loss_val, dtype=torch.float32)\n",
    "\n",
    "#         combined_loss = - pol_loss_t * chosen_log_prob + (new_value[0] - val_loss_t)**2\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         combined_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_pol_loss += pol_loss_val\n",
    "#         total_val_loss += val_loss_val\n",
    "#         steps_count += 1\n",
    "\n",
    "#         print(f\"  State={st}, ratio={ratio_val:.3f}, advantage={advantage:.2f}, action_idx={action_idx}\")\n",
    "#         print(f\"    pol_loss={pol_loss_val:.3f}, val_loss={val_loss_val:.3f}\")\n",
    "\n",
    "#     if steps_count > 0:\n",
    "#         print(f\"=> iteration {iter_idx} done. avg pol_loss={total_pol_loss/steps_count:.3f}, avg val_loss={total_val_loss/steps_count:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandResult:\n",
    "    def __init__(self, card_reps: list, action_reps: list, actions_taken:list, rewards: list, deltas: list):\n",
    "        if len(card_reps) != len(action_reps) or len(card_reps) != len(card_reps): # We need to make sure that the length of the card_reps, action_reps and rewards are the same. Action before showdown should be a None value or smth like that\n",
    "            print(len(card_reps), len(action_reps), len(rewards))\n",
    "            raise ValueError(\"card_reps, rewards and action_reps must have the same length\") \n",
    "        self.states = list((card_rep.card_tensor, action_reps.action_tensor) for card_rep, action_reps in zip(card_reps, action_reps))\n",
    "        self.rewards = rewards\n",
    "        self.rounds = [{\n",
    "            'state': self.states[i],\n",
    "            'action_taken': actions_taken[i],\n",
    "            'reward': rewards[i],\n",
    "            'deltas': deltas[i]\n",
    "        } for i in range(len(self.states))]\n",
    "\n",
    "    def new_state(self, card_rep: CardRepresentation, action_rep: ActionRepresentation, action_taken: int, reward: int, deltas: list):\n",
    "        self.states.append((card_rep, action_rep))\n",
    "        self.rewards.append(reward)\n",
    "        self.rounds.append({\n",
    "            'state': (card_rep.card_tensor, action_rep.action_tensor),\n",
    "            'action_taken': action_taken,\n",
    "            'reward': reward,\n",
    "            'deltas': deltas\n",
    "        })\n",
    "\n",
    "\n",
    "def get_hand_result(method1: bool) -> HandResult:\n",
    "    \"\"\"\n",
    "    Given a list of 'states' like ['Preflop','Flop','Turn','River','Showdown'],\n",
    "    build a numpy array of shape (5,1) for the siamese model.\n",
    "\n",
    "    I am sure both methods output the same result.\n",
    "    \"\"\"\n",
    "    states = ['Preflop','Flop','Turn','River']\n",
    "    # empty np.array with length 5\n",
    "    if (method1):\n",
    "        card_reps = [build_card_rep_for_state(state) for state in states]\n",
    "        action_reps = [build_action_rep_for_state(state) for state in states]\n",
    "        actions_taken = [get_action(state, 0)[3] for state in states]\n",
    "        rewards = [build_reward_for_state(state) for state in states]\n",
    "        deltas = [get_deltas(state) for state in states]\n",
    "        return HandResult(card_reps, action_reps, actions_taken, rewards, deltas)\n",
    "    else:\n",
    "        for state in states:\n",
    "            card_rep = build_card_rep_for_state(state)\n",
    "            action_rep = build_action_rep_for_state(state)\n",
    "            action_taken = get_action(state, 0)[3]\n",
    "            reward = build_reward_for_state(state)\n",
    "            deltas = get_deltas(state)\n",
    "            if state == 'Preflop':\n",
    "                hand_result = HandResult([card_rep], [action_rep], [action_taken], [reward], [deltas])\n",
    "            else:\n",
    "                hand_result.new_state(card_rep, action_rep, action_taken, reward, deltas)\n",
    "        return hand_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_iteration(iter_idx: int, rounds_array, old_policy_net: PseudoSiameseNet, new_policy_net: PseudoSiameseNet,\n",
    "                      optimizer: optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Demonstrates a single iteration (episode) with states=[Preflop,Flop,Turn,River,Showdown]\n",
    "    and rewards=[-20, -20, -80, 0, 240].\n",
    "    We'll do partial PPO logic: \n",
    "      - compute advantage,\n",
    "      - old/new policy ratio,\n",
    "      - trinal-clip policy loss,\n",
    "      - a toy value loss, \n",
    "      - gradient update => new_policy_net changes.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Iteration {iter_idx} ===\")\n",
    "    # states = ['Preflop','Flop','Turn','River','Showdown']\n",
    "    # rewards = [-20, -20, -80, 0, 240]\n",
    "\n",
    "    # We'll track some metrics\n",
    "    total_pol_loss = 0\n",
    "    total_val_loss = 0\n",
    "    steps_count = 0\n",
    "\n",
    "    # Build a model-based value function using the *new* net \n",
    "    # (In typical PPO, the value function is updated simultaneously with the new policy.)\n",
    "    model_value_func = make_model_value_function(new_policy_net)\n",
    "    states, rewards = zip(*[(round[\"state\"], round[\"reward\"]) for round in rounds_array])\n",
    "\n",
    "    for i, round in enumerate(rounds_array):  # skip Showdown itself\n",
    "\n",
    "        deltas = round['deltas']\n",
    "        action_taken = round['action_taken']\n",
    "        state = round['state']\n",
    "\n",
    "        # 1) Compute advantage\n",
    "        if i < len(rounds_array) - 1:\n",
    "            future_rewards = rewards[i:]\n",
    "            future_states  = states[i:]\n",
    "            advantage_t = a_gae(future_states, future_rewards, model_value_func, gamma=0.999, lambda_=0.99)\n",
    "\n",
    "        # 2) Build card/action reps for this state\n",
    "        card_tensor = state[0]\n",
    "        action_tensor = state[1]\n",
    "        action_t, card_t = to_torch_input(card_tensor, action_tensor)\n",
    "\n",
    "        # 3) old policy => old_probs\n",
    "        old_logits, _ = old_policy_net.forward(action_t, card_t) # I think dont want to use no_grad here\n",
    "        old_probs = logits_to_probs(old_logits)[0].detach().cpu().numpy()\n",
    "\n",
    "        # 4) new policy => new_probs + new_value\n",
    "        new_logits, new_value = new_policy_net.forward(action_t, card_t)\n",
    "        new_probs_t = logits_to_probs(new_logits)[0]\n",
    "        new_probs = new_probs_t.detach().cpu().numpy()\n",
    "\n",
    "        # 5) sample an action from the *new* policy. THIS IS THE ACTION_TAKEN\n",
    "        \n",
    "\n",
    "        # 6) ratio = new_probs[action_idx]/old_probs[action_idx]\n",
    "        ratio_t = ratio(old_probs, new_probs, action_taken)\n",
    "\n",
    "        # 7) policy loss\n",
    "        deltas = round['deltas']\n",
    "        pol_loss_t = tc_loss_function(ratio_t, advantage_t, epsilon=0.2, deltas=deltas)\n",
    "        pol_loss_val = pol_loss_t.item()\n",
    "\n",
    "        # 8) value loss\n",
    "        #    compute r_gamma from future rewards\n",
    "        r_g = r_gamma(np.array(future_rewards), gamma=0.999)\n",
    "        val_loss_t = v_loss(r_g, state, deltas, new_value)\n",
    "        val_loss_val = val_loss_t.item()\n",
    "\n",
    "        # 9) build a toy combined loss => do a gradient update\n",
    "        #   - incorporate pol_loss_val and val_loss_val into the PyTorch graph \n",
    "        #   - We'll do a negative log(prob_of_action) scaled by pol_loss_val,\n",
    "        #     plus MSE( new_value, val_loss_val ).\n",
    "        chosen_log_prob = torch.log(new_probs_t[action_taken] + 1e-8)\n",
    "        pol_loss_t = torch.tensor(pol_loss_val, dtype=torch.float32)\n",
    "        val_loss_t = torch.tensor(val_loss_val, dtype=torch.float32)\n",
    "\n",
    "        combined_loss = - pol_loss_t * chosen_log_prob + (new_value[0] - val_loss_t)**2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_pol_loss += pol_loss_val\n",
    "        total_val_loss += val_loss_val\n",
    "        steps_count += 1\n",
    "        print(f\"  State {i}, ratio={ratio_t.item():.3f}, advantage={advantage_t.item():.2f}, action_idx={action_taken}\")\n",
    "        print(f\"    pol_loss={pol_loss_val:.3f}, val_loss={val_loss_val:.3f}\")\n",
    "\n",
    "    if steps_count > 0:\n",
    "        print(f\"=> iteration {iter_idx} done. avg pol_loss={total_pol_loss/steps_count:.3f}, avg val_loss={total_val_loss/steps_count:.3f}\")\n",
    "\n",
    "\n",
    "def train_model(model, hand_results):\n",
    "    old_policy_net = PseudoSiameseNet()\n",
    "    new_policy_net = PseudoSiameseNet()\n",
    "    clone_model_weights(model, old_policy_net)\n",
    "    clone_model_weights(model, new_policy_net)\n",
    "    optimizer = optim.Adam(new_policy_net.parameters(), lr=0.001)\n",
    "    for i, hand_result in enumerate(hand_results):\n",
    "        new_policy_net_after_one_iteration = run_one_iteration(i, hand_result.rounds, old_policy_net, new_policy_net, optimizer)\n",
    "        clone_model_weights(new_policy_net, new_policy_net_after_one_iteration) # Check if we have to do it like this or it is enough to return new_policy_net directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the model plays and generates all the states representations. You get for each round:\n",
    "# - the card representations\n",
    "# - the action representations\n",
    "# - the rewards array\n",
    "\n",
    "# Then we run the PPO algorithm to update the policy and value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_result1 = get_hand_result(True)\n",
    "hand_result2 = get_hand_result(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Here I checked both methods were working equally\n",
    "key = 'state'\n",
    "for index in range(len(hand_result1.rounds)):\n",
    "    print((hand_result1.rounds[index][key][1] == hand_result2.rounds[index][key][1]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounds_array = hand_result2.rounds\n",
    "rounds_array[0][\"reward\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_result = get_hand_result(True)\n",
    "hand_results = [hand_result for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "  [1. 0. 1. 0. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 2. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0.]]] 240\n"
     ]
    }
   ],
   "source": [
    "# Here I wanted to manually check all the actions where being added correctly\n",
    "\n",
    "id = 3\n",
    "\n",
    "print(hand_result.rounds[id][\"state\"][1], hand_result.rounds[id][\"reward\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 0 ===\n",
      "  State 0, ratio=0.994, advantage=-1.77, action_idx=6\n",
      "    pol_loss=-1.760, val_loss=402.759\n",
      "  State 1, ratio=0.985, advantage=-1.19, action_idx=3\n",
      "    pol_loss=-1.171, val_loss=1609.318\n",
      "  State 2, ratio=0.974, advantage=-0.80, action_idx=1\n",
      "    pol_loss=-0.779, val_loss=6425.976\n",
      "  State 3, ratio=1.044, advantage=-0.80, action_idx=8\n",
      "    pol_loss=-0.836, val_loss=6433.449\n",
      "=> iteration 0 done. avg pol_loss=-1.136, avg val_loss=3717.876\n"
     ]
    }
   ],
   "source": [
    "old_policy = PseudoSiameseNet()\n",
    "new_policy = PseudoSiameseNet()\n",
    "run_one_iteration(0, hand_result.rounds, old_policy, new_policy, optim.Adam(new_policy.parameters(), lr=0.001))\n",
    "\n",
    "# THERES SOMETHING WRONG WITH THE SIAMESE ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Iteration 0 ===\n",
      "  State 0, ratio=1.000, advantage=-1.77, action_idx=6\n",
      "    pol_loss=-1.770, val_loss=399.866\n",
      "  State 1, ratio=1.005, advantage=-1.19, action_idx=3\n",
      "    pol_loss=-1.195, val_loss=1602.866\n",
      "  State 2, ratio=1.007, advantage=-0.80, action_idx=1\n",
      "    pol_loss=-0.806, val_loss=6412.286\n",
      "  State 3, ratio=1.004, advantage=-0.80, action_idx=8\n",
      "    pol_loss=-0.803, val_loss=6420.155\n",
      "=> iteration 0 done. avg pol_loss=-1.143, avg val_loss=3708.793\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'load_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m bot \u001b[38;5;241m=\u001b[39m PseudoSiameseNet()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhand_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[93], line 102\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, hand_results)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, hand_result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hand_results):\n\u001b[0;32m    101\u001b[0m     new_policy_net_after_one_iteration \u001b[38;5;241m=\u001b[39m run_one_iteration(i, hand_result\u001b[38;5;241m.\u001b[39mrounds, old_policy_net, new_policy_net, optimizer)\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mclone_model_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_policy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_policy_net_after_one_iteration\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mario\\Documents\\Git\\pokAI\\ai\\siamese_net.py:102\u001b[0m, in \u001b[0;36mclone_model_weights\u001b[1;34m(src_model, dst_model)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone_model_weights\u001b[39m(src_model: nn\u001b[38;5;241m.\u001b[39mModule, dst_model: nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    Copies parameters from src_model to dst_model (in-place).\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    This function is used to create an 'old policy' from a 'new policy'.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mdst_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m(src_model\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'load_state_dict'"
     ]
    }
   ],
   "source": [
    "bot = PseudoSiameseNet()\n",
    "train_model(bot, hand_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 4, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_result.rounds[0][\"state\"][1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
