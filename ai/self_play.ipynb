{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import urllib3\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Add project root to path so we can import modules from the api and ai folders.\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "# Disable HTTPS warnings.\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Import helper functions for building representations.\n",
    "# from api.replay import parse_card, build_replay_experiences\n",
    "# from experiment import build_action_rep_for_state, to_torch_input\n",
    "\n",
    "# Import our (dummy) API functions needed for self-play simulation.\n",
    "# (Note: We will not use NewHand or Act since we simulate self-play.)\n",
    "from api.playSlumbot import get_street_name, parse_action_enhanced, ChooseActionAI, index_to_action_string, STACK_SIZE, SMALL_BLIND, BIG_BLIND\n",
    "\n",
    "# Import our model and PPO utilities.\n",
    "from siamese_net import PseudoSiameseNet, logits_to_probs, clone_model_weights, to_torch_input\n",
    "from ppo_utils import a_gae, tc_loss_function, ratio, r_gamma, v_loss, make_model_value_function\n",
    "\n",
    "# Import HandResult grouping functions.\n",
    "from hand_result import create_hands_from_experiences, build_experiences_from_txt, build_replay_experiences\n",
    "\n",
    "# Set constants.\n",
    "NUM_STREETS = 4\n",
    "STACK_SIZE = 20000\n",
    "BIG_BLIND = 100\n",
    "SMALL_BLIND = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Define an AgentEntry class and a global history pool.\n",
    "INITIAL_ELO = 1500\n",
    "\n",
    "class AgentEntry:\n",
    "    def __init__(self, policy_net, elo=INITIAL_ELO):\n",
    "        self.policy_net = policy_net\n",
    "        self.elo = elo\n",
    "\n",
    "history_pool = []\n",
    "\n",
    "# Elo update helper functions.\n",
    "def expected_score(rating_A, rating_B):\n",
    "    return 1 / (1 + 10 ** ((rating_B - rating_A) / 400))\n",
    "\n",
    "def update_elo(rating, expected, score, k_factor=32):\n",
    "    return rating + k_factor * (score - expected)\n",
    "\n",
    "# Dummy simulation of a single hand between two agents.\n",
    "def simulate_hand(agentA, agentB):\n",
    "    \"\"\"\n",
    "    Simulates a single hand between agentA and agentB.\n",
    "    Returns a chip differential (positive means agentA wins chips).\n",
    "    Replace this with your actual hand simulation.\n",
    "    \"\"\"\n",
    "    # For demonstration, return a random chip differential between -3000 and 3000.\n",
    "    return np.random.randint(-3000, 3001)\n",
    "\n",
    "# Build a self-play hand that also returns a dummy replay experience.\n",
    "def self_play_hand(agentA, agentB):\n",
    "    \"\"\"\n",
    "    Simulates one hand between agentA and agentB.\n",
    "    Returns a tuple: (chip differential, experiences)\n",
    "    where experiences is a list of replay experience dictionaries produced by build_replay_experiences.\n",
    "    For demonstration, a dummy replay string is used.\n",
    "    \"\"\"\n",
    "    diff = simulate_hand(agentA, agentB)\n",
    "    # Dummy replay string (in your real system, this would come from simulating full play).\n",
    "    replay_str = \"b200c/kk/b200f\"\n",
    "    # Dummy hole cards and board.\n",
    "    hole_cards = ['As', 'Kd']\n",
    "    board = []  # Preflop (empty board)\n",
    "    client_pos = 0  # Assume agentA is hero.\n",
    "    # Build experiences using your build_replay_experiences function.\n",
    "    experiences, _, _, _ = build_replay_experiences(replay_str, board, hole_cards, client_pos)\n",
    "    for exp in experiences:\n",
    "        exp['winnings'] = diff\n",
    "    return diff, experiences\n",
    "\n",
    "def self_play_batch(num_hands, agentA, agentB):\n",
    "    \"\"\"\n",
    "    Simulates a batch of hands between agentA and agentB.\n",
    "    Returns the total chip differential (agentA perspective) and the concatenated replay experiences.\n",
    "    \"\"\"\n",
    "    total_diff = 0\n",
    "    all_experiences = []\n",
    "    for _ in range(num_hands):\n",
    "        diff, exps = self_play_hand(agentA, agentB)\n",
    "        total_diff += diff\n",
    "        all_experiences.extend(exps)\n",
    "    return total_diff, all_experiences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def play_batch(num_hands, token, policy_net, replay_path=\"replay.txt\"):\n",
    "    \"\"\"\n",
    "    In self-play, we simulate a batch of hands and write dummy replay data.\n",
    "    This function calls self_play_batch and writes the resulting experiences to replay_path.\n",
    "    Returns token (unused), total winnings, and average mBB/hand.\n",
    "    \"\"\"\n",
    "    total_winnings = 0\n",
    "    all_experiences = []\n",
    "    # Clear previous replay file.\n",
    "    open(replay_path, \"w\").close()\n",
    "    \n",
    "    for h in range(num_hands):\n",
    "        # Instead of PlayHand (API), we use our self-play simulation.\n",
    "        diff, exps = self_play_hand(policy_net, policy_net)  # self-play: agent plays against itself.\n",
    "        total_winnings += diff\n",
    "        all_experiences.extend(exps)\n",
    "        # Write a simplified line to replay file.\n",
    "        with open(replay_path, \"a\") as f:\n",
    "            # Format: hand_index,final_action,board,hole_cards,client_pos,winnings\n",
    "            # Here we use the dummy replay string from self_play_hand.\n",
    "            f.write(f\"{h+1},dummy_replay,{[]},{['As','Kd']},{0},{diff}\\n\")\n",
    "    \n",
    "    avg_bb = total_winnings / (num_hands * BIG_BLIND)\n",
    "    print(f\"\\nBatch DONE. Total winnings: {total_winnings}, mBB/hand: {avg_bb:.3f}\")\n",
    "    return token, total_winnings, avg_bb\n",
    "\n",
    "def run_one_iteration(iter_idx: int, rounds_array, old_policy_net, new_policy_net, optimizer):\n",
    "    \"\"\"\n",
    "    Executes a single PPO update iteration on the given rounds_array.\n",
    "    Returns average policy loss, average value loss, and number of steps.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Training Iteration {iter_idx} ===\")\n",
    "    total_pol_loss = 0\n",
    "    total_val_loss = 0\n",
    "    steps_count = 0\n",
    "\n",
    "    model_value_func = make_model_value_function(new_policy_net)\n",
    "    states, rewards = zip(*[(r[\"state\"], r[\"reward\"]) for r in rounds_array])\n",
    "    \n",
    "    for i, round_ in enumerate(rounds_array):\n",
    "        deltas = round_['deltas']\n",
    "        action_taken = round_['action_taken']\n",
    "        state = round_['state']\n",
    "        \n",
    "        if i < len(rounds_array) - 1:\n",
    "            future_rewards = rewards[i:]\n",
    "            future_states  = states[i:]\n",
    "            advantage_t = a_gae(future_states, future_rewards, model_value_func, gamma=0.999, lambda_=0.99)\n",
    "        else:\n",
    "            future_rewards = [round_[\"reward\"]]\n",
    "            advantage_t = torch.tensor(0.0)\n",
    "        \n",
    "        card_tensor = state[0]\n",
    "        action_tensor = state[1]\n",
    "        action_t, card_t = to_torch_input(card_tensor, action_tensor)\n",
    "        \n",
    "        old_logits, _ = old_policy_net.forward(action_t, card_t)\n",
    "        old_probs = logits_to_probs(old_logits)[0].detach().cpu().numpy()\n",
    "\n",
    "        new_logits, new_value = new_policy_net.forward(action_t, card_t)\n",
    "        new_probs_t = logits_to_probs(new_logits)[0]\n",
    "        new_probs = new_probs_t.detach().cpu().numpy()\n",
    "\n",
    "        ratio_t = ratio(old_probs, new_probs, action_taken)\n",
    "        pol_loss_tensor = tc_loss_function(ratio_t, advantage_t, epsilon=0.2)\n",
    "        pol_loss_val = pol_loss_tensor.item()\n",
    "\n",
    "        r_g = r_gamma(np.array(future_rewards), gamma=0.999)\n",
    "        val_loss_tensor = v_loss(r_g, state, deltas, new_value)\n",
    "        val_loss_val = val_loss_tensor.item()\n",
    "\n",
    "        # For simplicity, combine losses linearly.\n",
    "        c = 1\n",
    "        combined_loss = -pol_loss_tensor + c * val_loss_tensor\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_pol_loss += pol_loss_val\n",
    "        total_val_loss += val_loss_val\n",
    "        steps_count += 1\n",
    "        print(f\"  Iter {iter_idx} round {i}: pol_loss={pol_loss_val:.3f}, val_loss={val_loss_val:.3f}\")\n",
    "    \n",
    "    avg_pol = total_pol_loss / steps_count if steps_count > 0 else 0\n",
    "    avg_val = total_val_loss / steps_count if steps_count > 0 else 0\n",
    "    print(f\"=> Iteration {iter_idx} done. avg pol_loss={avg_pol:.3f}, avg val_loss={avg_val:.3f}\")\n",
    "    return avg_pol, avg_val, steps_count\n",
    "\n",
    "def train_model(model, hand_results):\n",
    "    \"\"\"\n",
    "    Trains the model on the provided hand_results.\n",
    "    Returns the updated model along with overall average policy and value losses.\n",
    "    \"\"\"\n",
    "    old_policy_net = PseudoSiameseNet()\n",
    "    new_policy_net = PseudoSiameseNet()\n",
    "    clone_model_weights(model, old_policy_net)\n",
    "    clone_model_weights(model, new_policy_net)\n",
    "    optimizer = optim.Adam(new_policy_net.parameters(), lr=0.0001)\n",
    "    \n",
    "    total_pol_loss = 0\n",
    "    total_val_loss = 0\n",
    "    total_steps = 0\n",
    "    for i, hand_result in enumerate(hand_results):\n",
    "        avg_pol, avg_val, steps = run_one_iteration(i, hand_result.rounds, old_policy_net, new_policy_net, optimizer)\n",
    "        total_pol_loss += avg_pol * steps\n",
    "        total_val_loss += avg_val * steps\n",
    "        total_steps += steps\n",
    "    clone_model_weights(new_policy_net, old_policy_net)\n",
    "    overall_avg_pol = total_pol_loss / total_steps if total_steps > 0 else 0\n",
    "    overall_avg_val = total_val_loss / total_steps if total_steps > 0 else 0\n",
    "    return new_policy_net, overall_avg_pol, overall_avg_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Self-play: use our dummy simulation to generate chip differentials and dummy replay experiences.\n",
    "def simulate_selfplay_match(num_hands, agentA, agentB):\n",
    "    return self_play_batch(num_hands, agentA, agentB)\n",
    "\n",
    "# Function to select an opponent from the top K in the history pool.\n",
    "def select_opponent(history_pool, K=5):\n",
    "    sorted_pool = sorted(history_pool, key=lambda entry: entry.elo, reverse=True)\n",
    "    top_K = sorted_pool[:K]\n",
    "    return random.choice(top_K).policy_net\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Initialize current agent and history pool.\n",
    "current_agent = PseudoSiameseNet()\n",
    "current_agent.elo = INITIAL_ELO\n",
    "history_pool.append(AgentEntry(current_agent, elo=INITIAL_ELO))\n",
    "\n",
    "# Continuous training loop.\n",
    "num_cycles = 10\n",
    "match_hands = 100       # number of hands per self-play match (for Elo update)\n",
    "num_matches_per_cycle = 5  # number of matches per cycle (each match is 100 hands)\n",
    "hands_per_cycle = 100   # number of hands for generating training replays\n",
    "\n",
    "# Metrics logging.\n",
    "avg_bb_list = []         # average mBB/hand from self-play matches\n",
    "avg_pol_loss_list = []   # average policy loss per cycle\n",
    "avg_val_loss_list = []   # average value loss per cycle\n",
    "elo_history = []\n",
    "cycle_numbers = []\n",
    "\n",
    "for cycle in range(num_cycles):\n",
    "    print(f\"\\n=== Cycle {cycle+1}/{num_cycles} ===\")\n",
    "    \n",
    "    # 1. Self-play: select an opponent from history.\n",
    "    opponent_agent = select_opponent(history_pool, K=5)\n",
    "    \n",
    "    # Run several self-play matches to determine chip differential.\n",
    "    match_diffs = []\n",
    "    for m in range(num_matches_per_cycle):\n",
    "        diff, _ = simulate_selfplay_match(match_hands, current_agent, opponent_agent)\n",
    "        match_diffs.append(diff)\n",
    "        print(f\"  Match {m+1}: chip differential = {diff}\")\n",
    "    avg_chip_diff = np.mean(match_diffs)\n",
    "    mBB_per_hand = avg_chip_diff / (BIG_BLIND * match_hands)\n",
    "    avg_bb_list.append(mBB_per_hand)\n",
    "    print(f\"Cycle {cycle+1}: Average mBB/hand = {mBB_per_hand:.3f}\")\n",
    "    \n",
    "    # 2. Determine match result: win if chip diff >= +10000, loss if <= -10000, draw otherwise.\n",
    "    if avg_chip_diff >= 10000:\n",
    "        result = 1.0\n",
    "    elif avg_chip_diff <= -10000:\n",
    "        result = 0.0\n",
    "    else:\n",
    "        result = 0.5\n",
    "    \n",
    "    R_current = current_agent.elo\n",
    "    R_opponent = opponent_agent.elo if hasattr(opponent_agent, \"elo\") else INITIAL_ELO\n",
    "    exp_current = expected_score(R_current, R_opponent)\n",
    "    exp_opponent = expected_score(R_opponent, R_current)\n",
    "    new_R_current = update_elo(R_current, exp_current, result, k_factor=32)\n",
    "    new_R_opponent = update_elo(R_opponent, exp_opponent, 1 - result, k_factor=32)\n",
    "    current_agent.elo = new_R_current\n",
    "    opponent_agent.elo = new_R_opponent\n",
    "    elo_history.append(new_R_current)\n",
    "    print(f\"Cycle {cycle+1}: Updated Elo: current = {new_R_current:.1f}, opponent = {new_R_opponent:.1f}\")\n",
    "    \n",
    "    # 3. Generate training experiences from self-play.\n",
    "    # Here we simulate a batch of self-play hands (which writes dummy replay data to file).\n",
    "    token = None  # not used in self-play simulation.\n",
    "    token, _, _ = play_batch(hands_per_cycle, token, current_agent, replay_path=\"replay.txt\")\n",
    "    \n",
    "    # 4. Build experiences and group into hand results.\n",
    "    experiences = build_experiences_from_txt(\"replay.txt\")\n",
    "    print(f\"Cycle {cycle+1}: Loaded {len(experiences)} experiences.\")\n",
    "    hand_results = create_hands_from_experiences(experiences)\n",
    "    print(f\"Cycle {cycle+1}: Created {len(hand_results)} hand results.\")\n",
    "    \n",
    "    # 5. Train on these experiences.\n",
    "    policy_net, avg_pol, avg_val = train_model(current_agent, hand_results)\n",
    "    avg_pol_loss_list.append(avg_pol)\n",
    "    avg_val_loss_list.append(avg_val)\n",
    "    cycle_numbers.append(cycle+1)\n",
    "    \n",
    "    current_agent = policy_net  # update current agent.\n",
    "    \n",
    "    # 6. Add the new version of the current agent to the history pool.\n",
    "    history_pool.append(AgentEntry(deepcopy(current_agent), elo=current_agent.elo))\n",
    "    \n",
    "    # 7. Clear replay file for next cycle.\n",
    "    open(\"replay.txt\", \"w\").close()\n",
    "    \n",
    "    print(f\"Cycle {cycle+1} complete: mBB/hand = {mBB_per_hand:.3f}, avg policy loss = {avg_pol:.3f}, avg value loss = {avg_val:.3f}\")\n",
    "\n",
    "# Plot metrics.\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(cycle_numbers, avg_bb_list, marker='o', label=\"mBB/hand\")\n",
    "plt.xlabel(\"Cycle\")\n",
    "plt.ylabel(\"mBB/hand\")\n",
    "plt.title(\"Average mBB/hand Over Cycles\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(cycle_numbers, avg_pol_loss_list, marker='o', label=\"Avg Policy Loss\")\n",
    "plt.xlabel(\"Cycle\")\n",
    "plt.ylabel(\"Policy Loss\")\n",
    "plt.title(\"Avg Policy Loss Over Cycles\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(cycle_numbers, avg_val_loss_list, marker='o', label=\"Avg Value Loss\")\n",
    "plt.xlabel(\"Cycle\")\n",
    "plt.ylabel(\"Value Loss\")\n",
    "plt.title(\"Avg Value Loss Over Cycles\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(cycle_numbers, elo_history, marker='o', label=\"Current Agent Elo\")\n",
    "plt.xlabel(\"Cycle\")\n",
    "plt.ylabel(\"ELO\")\n",
    "plt.title(\"Current Agent Elo Over Cycles\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
