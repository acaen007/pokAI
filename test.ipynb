{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_gamma(rewards: np.array, gamma):\n",
    "    r_gamma = 0\n",
    "    for reward in rewards[:0:-1]:  # Slicing to reverse except the first element\n",
    "        r_gamma = gamma * (r_gamma + reward)\n",
    "        print(reward)\n",
    "    r_gamma += rewards[0]\n",
    "    return r_gamma\n",
    "\n",
    "def value_function(state): #TODO: implement this\n",
    "    return 0\n",
    "\n",
    "def v_loss(r_gamma, state, deltas):\n",
    "    return (np.clip(r_gamma, -deltas[1], deltas[2])-value_function(state))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    if state == 'Preflop':\n",
    "        # return np.array([0.5, 0.5, 0, 0, 0])\n",
    "        return np.array([0, 1, 0, 0, 0])\n",
    "    elif state == 'Flop':\n",
    "        # return np.array([0, 0.5, 0.5, 0, 0])\n",
    "        return np.array([0, 0, 1, 0, 0])\n",
    "    elif state == 'Turn':\n",
    "        # return np.array([0, 0, 1/3, 1/3, 1/3])\n",
    "        return np.array([0, 0, 0, 1, 0])\n",
    "    elif state == 'River':\n",
    "        # return np.array([0, 1/2, 1/2, 0, 0])\n",
    "        return np.array([0, 0, 1, 0, 0])\n",
    "    return 0\n",
    "\n",
    "def get_deltas(state): # I'm not sure wether the deltas should add the chips from previous states or just the current state\n",
    "    delta1 = 3\n",
    "    if state == 'Preflop':\n",
    "        delta2 = 20\n",
    "        delta3 = 10 # The opponent put in the big blind and the agent just betted. The opponnent hasnt put in any chips yet.\n",
    "    elif state == 'Flop':\n",
    "        delta2 = 40\n",
    "        delta3 = 20\n",
    "    elif state == 'Turn':\n",
    "        delta2 = 120\n",
    "        delta3 = 80\n",
    "    elif state == 'River':\n",
    "        delta2 = 120\n",
    "        delta3 = 120\n",
    "    return delta1, delta2, delta3\n",
    "\n",
    "def ratio(old_policy, new_policy, action, state):\n",
    "    return new_policy(state)[action] / old_policy(state)[action]\n",
    "\n",
    "def a_gae(results, states, value_function, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Generalized Advantage Estimator (GAE) where:\n",
    "      - len(states) == len(results)\n",
    "      - We do NOT assume an extra 'terminal state' beyond these states.\n",
    "    \n",
    "    results:       list/array of rewards at each timestep\n",
    "    states:        list/array of states at each timestep\n",
    "    value_function: function that takes a state and returns a scalar value\n",
    "    gamma:         discount factor\n",
    "    lambda_:       GAE parameter\n",
    "    \"\"\"\n",
    "    N = len(results)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # For convenience, compute V(s0) once\n",
    "    v0 = value_function(states[0])\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # 1) Precompute partial sums of discounted rewards:\n",
    "    #    S[k] = sum_{i=0..k-1} gamma^i * results[i], with S[0] = 0\n",
    "    #\n",
    "    #    Then the \"raw\" advantage term (before weighting by λ^(k-1)) is:\n",
    "    #       a_k = - V(s0) + S[k] + gamma^k * V(sk),\n",
    "    #    for k in 1..N-1 (because states[k] must be valid).\n",
    "    # --------------------------------------------------------\n",
    "    S = np.zeros(N+1, dtype=float)\n",
    "    for i in range(N):\n",
    "        S[i+1] = S[i] + (gamma ** i) * results[i]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Accumulate the GAE sum:\n",
    "    #\n",
    "    #    A = (1 - λ) * Σ (λ^(k-1) * a_k), for k = 1..N-1\n",
    "    #\n",
    "    #    We use k=1..N-1 so that states[k] is still in range.\n",
    "    # --------------------------------------------------------\n",
    "    gae_sum = 0.0\n",
    "    for k in range(1, N):\n",
    "        a_k = -v0 + S[k] + (gamma ** k) * value_function(states[k])\n",
    "        gae_sum += (lambda_ ** (k - 1)) * a_k\n",
    "    \n",
    "    return (1 - lambda_) * gae_sum\n",
    "\n",
    "# I wasn't sure how to treat the showdown state. The approach I am following is when the only states that are fed to the a_gae function are the river and the showdown, the resulting a_k() is \n",
    "# -V(river) + r(river) + V(showdown). I think the values for the river state and the showdown state will be different because the showdown value depends on the amount of chips that the agent has played\n",
    "# in the river state.\n",
    "    \n",
    "def tc_loss_function(ratio, advantage, epsilon, deltas): #We compute this for every hand and then average it\n",
    "    return np.clip(ratio, np.clip(ratio, 1 - epsilon, 1 + epsilon), deltas[0]) * advantage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(policy: callable, state):\n",
    "    return np.random.choice(len(policy(state)), p=policy(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "-100\n",
      "0\n",
      "-40\n",
      "159.06161882032\n",
      "320\n",
      "-100\n",
      "0\n",
      "-40\n",
      "25300.598581740778\n"
     ]
    }
   ],
   "source": [
    "rewards = np.array([-20, -40, 0, -100, 320])\n",
    "deltas = [3, 160, 160]\n",
    "print(r_gamma(rewards, 0.999))\n",
    "print(v_loss(r_gamma(rewards, 0.999), 0, deltas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(states, rewards, policy, value_function):\n",
    "    tc_loss = 0\n",
    "    value_loss = 0\n",
    "    states_without_showdown = states[:-1]\n",
    "    for i, state in enumerate(states_without_showdown):\n",
    "        deltas = get_deltas(state)\n",
    "        rewards_from_now = rewards[i:]\n",
    "        states_from_now = states[i:]\n",
    "        advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99) #I'm not sure if this is correct\n",
    "        action = get_action(policy, state)\n",
    "        old_policy = policy\n",
    "        new_policy = policy\n",
    "        r = ratio(old_policy, new_policy, action, state)\n",
    "        tc_loss += tc_loss_function(r, advantage, 0.2, deltas)\n",
    "        value_loss += v_loss(r_gamma(rewards_from_now, 0.999), 0, deltas)\n",
    "\n",
    "    tc_loss /= len(states_without_showdown)\n",
    "    value_loss /= len(states_without_showdown)\n",
    "    return tc_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
      "1 [-20, -20, -80, 0, 240]\n",
      "1 -20.0\n",
      "2 [-20, -20, -80, 0, 240]\n",
      "2 -39.980000000000004\n",
      "3 [-20, -20, -80, 0, 240]\n",
      "3 -119.82008\n",
      "4 [-20, -20, -80, 0, 240]\n",
      "4 -119.82008\n",
      "Advantage:  -2.932771642119203\n",
      "20 10\n",
      "240\n",
      "0\n",
      "-80\n",
      "-20\n",
      "['Flop', 'Turn', 'River', 'Showdown']\n",
      "1 [-20, -80, 0, 240]\n",
      "1 -20.0\n",
      "2 [-20, -80, 0, 240]\n",
      "2 -99.92\n",
      "3 [-20, -80, 0, 240]\n",
      "3 -99.92\n",
      "Advantage:  -2.168523920000002\n",
      "40 20\n",
      "240\n",
      "0\n",
      "-80\n",
      "['Turn', 'River', 'Showdown']\n",
      "1 [-80, 0, 240]\n",
      "1 -80.0\n",
      "2 [-80, 0, 240]\n",
      "2 -80.0\n",
      "Advantage:  -1.5920000000000012\n",
      "120 80\n",
      "240\n",
      "0\n",
      "['River', 'Showdown']\n",
      "1 [0, 240]\n",
      "1 0.0\n",
      "Advantage:  0.0\n",
      "120 120\n",
      "240\n",
      "TC loss:  -1.6733238905298016\n",
      "Value loss:  5325.0\n"
     ]
    }
   ],
   "source": [
    "states = ['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
    "rewards = [-20, -20, -80, 0, 240] #There should be one more reward than states\n",
    "tc_loss = 0\n",
    "value_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "states_without_showdown = states[:-1]\n",
    "for i, state in enumerate(states_without_showdown):\n",
    "    deltas = get_deltas(state)\n",
    "    print(states[i:])\n",
    "    rewards_from_now = rewards[i:]\n",
    "    states_from_now = states[i:]\n",
    "    advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99) #I'm not sure if this is correct\n",
    "    print('Advantage: ', advantage)\n",
    "    action = get_action(policy, state)\n",
    "    old_policy = policy\n",
    "    new_policy = policy\n",
    "    r = ratio(old_policy, new_policy, action, state)\n",
    "    print(deltas[1], deltas[2])\n",
    "    tc_loss += tc_loss_function(r, advantage, 0.2, deltas)\n",
    "    value_loss += v_loss(r_gamma(rewards_from_now, 0.999), 0, deltas)\n",
    "\n",
    "tc_loss /= len(states_without_showdown)\n",
    "value_loss /= len(states_without_showdown)\n",
    "print('TC loss: ', tc_loss)\n",
    "print('Value loss: ', value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
      "Advantage:  -2.932771642119203\n",
      "20 10\n",
      "240\n",
      "0\n",
      "-80\n",
      "-20\n",
      "['Flop', 'Turn', 'River', 'Showdown']\n",
      "Advantage:  -2.168523920000002\n",
      "40 20\n",
      "240\n",
      "0\n",
      "-80\n",
      "['Turn', 'River', 'Showdown']\n",
      "Advantage:  -1.5920000000000012\n",
      "120 80\n",
      "240\n",
      "0\n",
      "['River', 'Showdown']\n",
      "Advantage:  0.0\n",
      "120 120\n",
      "240\n",
      "TC loss:  -1.6733238905298016\n",
      "Value loss:  5325.0\n"
     ]
    }
   ],
   "source": [
    "states = ['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
    "rewards = [-20, -20, -80, 0, 240] #There should be one more reward than states\n",
    "tc_loss = 0\n",
    "value_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "states_without_showdown = states[:-1]\n",
    "for i, state in enumerate(states_without_showdown):\n",
    "    deltas = get_deltas(state)\n",
    "    print(states[i:])\n",
    "    rewards_from_now = rewards[i:]\n",
    "    states_from_now = states[i:]\n",
    "    advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99) #I'm not sure if this is correct\n",
    "    print('Advantage: ', advantage)\n",
    "    action = get_action(policy, state)\n",
    "    old_policy = policy\n",
    "    new_policy = policy\n",
    "    r = ratio(old_policy, new_policy, action, state)\n",
    "    print(deltas[1], deltas[2])\n",
    "    tc_loss += tc_loss_function(r, advantage, 0.2, deltas)\n",
    "    value_loss += v_loss(r_gamma(rewards_from_now, 0.999), 0, deltas)\n",
    "\n",
    "tc_loss /= len(states_without_showdown)\n",
    "value_loss /= len(states_without_showdown)\n",
    "print('TC loss: ', tc_loss)\n",
    "print('Value loss: ', value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [-20, -20, -80, 0, 240]\n",
      "2 [-20, -20, -80, 0, 240]\n",
      "3 [-20, -20, -80, 0, 240]\n",
      "4 [-20, -20, -80, 0, 240]\n",
      "240\n",
      "0\n",
      "-80\n",
      "-20\n",
      "1 [-20, -80, 0, 240]\n",
      "2 [-20, -80, 0, 240]\n",
      "3 [-20, -80, 0, 240]\n",
      "240\n",
      "0\n",
      "-80\n",
      "1 [-80, 0, 240]\n",
      "2 [-80, 0, 240]\n",
      "240\n",
      "0\n",
      "1 [0, 240]\n",
      "240\n",
      "(np.float64(-0.9304432345098008), np.float64(5325.0))\n"
     ]
    }
   ],
   "source": [
    "print(get_losses(states, rewards, policy, value_function))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
