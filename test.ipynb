{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_gamma(rewards: np.array, gamma):\n",
    "    r_gamma = 0\n",
    "    for reward in rewards[:0:-1]:  # Slicing to reverse except the first element\n",
    "        r_gamma = gamma * (r_gamma + reward)\n",
    "        print(\"reward: \", reward)\n",
    "    r_gamma += rewards[0]\n",
    "    return r_gamma\n",
    "\n",
    "def value_function(state): #TODO: implement this\n",
    "    return 0\n",
    "\n",
    "def v_loss(r_gamma, state, deltas):\n",
    "    return (np.clip(r_gamma, -deltas[1], deltas[2])-value_function(state))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    if state == 'Preflop':\n",
    "        # return np.array([0.5, 0.5, 0, 0, 0])\n",
    "        return np.array([0, 1, 0, 0, 0])\n",
    "    elif state == 'Flop':\n",
    "        # return np.array([0, 0.5, 0.5, 0, 0])\n",
    "        return np.array([0, 0, 1, 0, 0])\n",
    "    elif state == 'Turn':\n",
    "        # return np.array([0, 0, 1/3, 1/3, 1/3])\n",
    "        return np.array([0, 0, 0, 1, 0])\n",
    "    elif state == 'River':\n",
    "        # return np.array([0, 1/2, 1/2, 0, 0])\n",
    "        return np.array([0, 0, 1, 0, 0])\n",
    "    return 0\n",
    "\n",
    "def get_deltas(state): # I'm not sure wether the deltas should add the chips from previous states or just the current state\n",
    "    delta1 = 3\n",
    "    if state == 'Preflop':\n",
    "        delta2 = 20\n",
    "        delta3 = 10 # The opponent put in the big blind and the agent just betted. The opponnent hasnt put in any chips yet.\n",
    "    elif state == 'Flop':\n",
    "        delta2 = 40\n",
    "        delta3 = 20\n",
    "    elif state == 'Turn':\n",
    "        delta2 = 120\n",
    "        delta3 = 80\n",
    "    elif state == 'River':\n",
    "        delta2 = 120\n",
    "        delta3 = 120\n",
    "    return delta1, delta2, delta3\n",
    "\n",
    "def ratio(old_policy, new_policy, action, state):\n",
    "    return new_policy(state)[action] / old_policy(state)[action]\n",
    "\n",
    "def a_gae(results, states, value_function, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Generalized Advantage Estimator (GAE) where:\n",
    "      - len(states) == len(results)\n",
    "      - We do NOT assume an extra 'terminal state' beyond these states.\n",
    "    \n",
    "    results:       list/array of rewards at each timestep\n",
    "    states:        list/array of states at each timestep\n",
    "    value_function: function that takes a state and returns a scalar value\n",
    "    gamma:         discount factor\n",
    "    lambda_:       GAE parameter\n",
    "    \"\"\"\n",
    "    N = len(results)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # For convenience, compute V(s0) once\n",
    "    v0 = value_function(states[0])\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # 1) Precompute partial sums of discounted rewards:\n",
    "    #    S[k] = sum_{i=0..k-1} gamma^i * results[i], with S[0] = 0\n",
    "    #\n",
    "    #    Then the \"raw\" advantage term (before weighting by λ^(k-1)) is:\n",
    "    #       a_k = - V(s0) + S[k] + gamma^k * V(sk),\n",
    "    #    for k in 1..N-1 (because states[k] must be valid).\n",
    "    # --------------------------------------------------------\n",
    "    S = np.zeros(N+1, dtype=float)\n",
    "    for i in range(N):\n",
    "        S[i+1] = S[i] + (gamma ** i) * results[i]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Accumulate the GAE sum:\n",
    "    #\n",
    "    #    A = (1 - λ) * Σ (λ^(k-1) * a_k), for k = 1..N-1\n",
    "    #\n",
    "    #    We use k=1..N-1 so that states[k] is still in range.\n",
    "    # --------------------------------------------------------\n",
    "    gae_sum = 0.0\n",
    "    for k in range(1, N):\n",
    "        a_k = -v0 + S[k] + (gamma ** k) * value_function(states[k])\n",
    "        gae_sum += (lambda_ ** (k - 1)) * a_k\n",
    "    \n",
    "    return (1 - lambda_) * gae_sum\n",
    "\n",
    "# I wasn't sure how to treat the showdown state. The approach I am following is when the only states that are fed to the a_gae function are the river and the showdown, the resulting a_k() is \n",
    "# -V(river) + r(river) + V(showdown). I think the values for the river state and the showdown state will be different because the showdown value depends on the amount of chips that the agent has played\n",
    "# in the river state.\n",
    "    \n",
    "def tc_loss_function(ratio, advantage, epsilon, deltas): #We compute this for every hand and then average it\n",
    "    return np.clip(ratio, np.clip(ratio, 1 - epsilon, 1 + epsilon), deltas[0]) * advantage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(policy: callable, state):\n",
    "    return np.random.choice(len(policy(state)), p=policy(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward:  320\n",
      "reward:  -100\n",
      "reward:  0\n",
      "reward:  -40\n",
      "159.06161882032\n",
      "reward:  320\n",
      "reward:  -100\n",
      "reward:  0\n",
      "reward:  -40\n",
      "25300.598581740778\n"
     ]
    }
   ],
   "source": [
    "rewards = np.array([-20, -40, 0, -100, 320])\n",
    "deltas = [3, 160, 160]\n",
    "print(r_gamma(rewards, 0.999))\n",
    "print(v_loss(r_gamma(rewards, 0.999), 0, deltas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(states, rewards, policy, value_function):\n",
    "    tc_loss = 0\n",
    "    value_loss = 0\n",
    "    states_without_showdown = states[:-1]\n",
    "    for i, state in enumerate(states_without_showdown):\n",
    "        deltas = get_deltas(state)\n",
    "        rewards_from_now = rewards[i:]\n",
    "        states_from_now = states[i:]\n",
    "        advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99) #I'm not sure if this is correct\n",
    "        action = get_action(policy, state)\n",
    "        old_policy = policy\n",
    "        new_policy = policy\n",
    "        r = ratio(old_policy, new_policy, action, state)\n",
    "        tc_loss += tc_loss_function(r, advantage, 0.2, deltas)\n",
    "        value_loss += v_loss(r_gamma(rewards_from_now, 0.999), 0, deltas)\n",
    "\n",
    "    tc_loss /= len(states_without_showdown)\n",
    "    value_loss /= len(states_without_showdown)\n",
    "    return tc_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
      "Advantage:  -2.932771642119203\n",
      "20 10\n",
      "240\n",
      "0\n",
      "-80\n",
      "-20\n",
      "['Flop', 'Turn', 'River', 'Showdown']\n",
      "Advantage:  -2.168523920000002\n",
      "40 20\n",
      "240\n",
      "0\n",
      "-80\n",
      "['Turn', 'River', 'Showdown']\n",
      "Advantage:  -1.5920000000000012\n",
      "120 80\n",
      "240\n",
      "0\n",
      "['River', 'Showdown']\n",
      "Advantage:  0.0\n",
      "120 120\n",
      "240\n",
      "TC loss:  -1.6733238905298016\n",
      "Value loss:  5325.0\n"
     ]
    }
   ],
   "source": [
    "states = ['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
    "rewards = [-20, -20, -80, 0, 240] #There should be one more reward than states\n",
    "tc_loss = 0\n",
    "value_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "states_without_showdown = states[:-1]\n",
    "for i, state in enumerate(states_without_showdown):\n",
    "    deltas = get_deltas(state)\n",
    "    print(states[i:])\n",
    "    print('deltas: ', deltas)\n",
    "    rewards_from_now = rewards[i:]\n",
    "    states_from_now = states[i:]\n",
    "    advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99) #I'm not sure if this is correct\n",
    "    print('Advantage: ', advantage)\n",
    "    action = get_action(policy, state)\n",
    "    old_policy = policy\n",
    "    new_policy = policy\n",
    "    r = ratio(old_policy, new_policy, action, state)\n",
    "    print(deltas[1], deltas[2])\n",
    "    tc_loss += tc_loss_function(r, advantage, 0.2, deltas)\n",
    "    value_loss += v_loss(r_gamma(rewards_from_now, 0.999), 0, deltas)\n",
    "\n",
    "tc_loss /= len(states_without_showdown)\n",
    "value_loss /= len(states_without_showdown)\n",
    "print('TC loss: ', tc_loss)\n",
    "print('Value loss: ', value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
      "Advantage:  -2.932771642119203\n",
      "20 10\n",
      "240\n",
      "0\n",
      "-80\n",
      "-20\n",
      "['Flop', 'Turn', 'River', 'Showdown']\n",
      "Advantage:  -2.168523920000002\n",
      "40 20\n",
      "240\n",
      "0\n",
      "-80\n",
      "['Turn', 'River', 'Showdown']\n",
      "Advantage:  -1.5920000000000012\n",
      "120 80\n",
      "240\n",
      "0\n",
      "['River', 'Showdown']\n",
      "Advantage:  0.0\n",
      "120 120\n",
      "240\n",
      "TC loss:  -1.6733238905298016\n",
      "Value loss:  5325.0\n"
     ]
    }
   ],
   "source": [
    "states = ['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
    "rewards = [-20, -20, -80, 0, 240] #There should be one more reward than states\n",
    "tc_loss = 0\n",
    "value_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "states_without_showdown = states[:-1]\n",
    "for i, state in enumerate(states_without_showdown):\n",
    "    deltas = get_deltas(state)\n",
    "    print(states[i:])\n",
    "    rewards_from_now = rewards[i:]\n",
    "    states_from_now = states[i:]\n",
    "    advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99) #I'm not sure if this is correct\n",
    "    print('Advantage: ', advantage)\n",
    "    action = get_action(policy, state)\n",
    "    old_policy = policy\n",
    "    new_policy = policy\n",
    "    r = ratio(old_policy, new_policy, action, state)\n",
    "    print(deltas[1], deltas[2])\n",
    "    tc_loss += tc_loss_function(r, advantage, 0.2, deltas)\n",
    "    value_loss += v_loss(r_gamma(rewards_from_now, 0.999), 0, deltas)\n",
    "\n",
    "tc_loss /= len(states_without_showdown)\n",
    "value_loss /= len(states_without_showdown)\n",
    "print('TC loss: ', tc_loss)\n",
    "print('Value loss: ', value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [-20, -20, -80, 0, 240]\n",
      "2 [-20, -20, -80, 0, 240]\n",
      "3 [-20, -20, -80, 0, 240]\n",
      "4 [-20, -20, -80, 0, 240]\n",
      "240\n",
      "0\n",
      "-80\n",
      "-20\n",
      "1 [-20, -80, 0, 240]\n",
      "2 [-20, -80, 0, 240]\n",
      "3 [-20, -80, 0, 240]\n",
      "240\n",
      "0\n",
      "-80\n",
      "1 [-80, 0, 240]\n",
      "2 [-80, 0, 240]\n",
      "240\n",
      "0\n",
      "1 [0, 240]\n",
      "240\n",
      "(np.float64(-0.9304432345098008), np.float64(5325.0))\n"
     ]
    }
   ],
   "source": [
    "print(get_losses(states, rewards, policy, value_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game State Representation\n",
    "\n",
    "## Card representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CardRepresentation:\n",
    "    \"\"\"\n",
    "    Incrementally build a 6 x 4 x 13 card tensor:\n",
    "      - Channel 0: hole cards\n",
    "      - Channel 1: flop\n",
    "      - Channel 2: turn\n",
    "      - Channel 3: river\n",
    "      - Channel 4: all public (flop+turn+river)\n",
    "      - Channel 5: hole + public\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Create it once, fill incrementally\n",
    "        self.card_tensor = np.zeros((6, 4, 13), dtype=np.float32)\n",
    "        \n",
    "        # Track which cards have been seen\n",
    "        self.hole_cards = []\n",
    "        self.public_cards = []\n",
    "    \n",
    "    def _mark_card(self, channel, rank, suit):\n",
    "        \"\"\"Helper to set a 1 for (channel, suit, rank).\"\"\"\n",
    "        self.card_tensor[channel, suit, rank] = 1.0\n",
    "    \n",
    "    def set_preflop(self, hole_cards):\n",
    "        \"\"\"\n",
    "        hole_cards: list of 2 tuples [(rank, suit), (rank, suit)]\n",
    "        Fills channel 0 (hole) and partially updates channel 5 (hole+public).\n",
    "        \"\"\"\n",
    "        self.hole_cards = hole_cards[:]  # store\n",
    "        for (r, s) in hole_cards:\n",
    "            self._mark_card(0, r, s)  # Channel 0: hole cards\n",
    "            self._mark_card(5, r, s)  # Channel 5: hole+public (so far, just hole)\n",
    "    \n",
    "    def set_flop(self, flop_cards):\n",
    "        \"\"\"\n",
    "        flop_cards: list of 3 tuples [(rank, suit), ...]\n",
    "        Fills channel 1 (flop), channel 4 (all public), channel 5 (hole+public).\n",
    "        \"\"\"\n",
    "        for (r, s) in flop_cards:\n",
    "            self._mark_card(1, r, s)  # Channel 1: flop\n",
    "            self._mark_card(4, r, s)  # Channel 4: public\n",
    "            self._mark_card(5, r, s)  # Channel 5: hole+public\n",
    "        self.public_cards.extend(flop_cards)\n",
    "    \n",
    "    def set_turn(self, turn_card):\n",
    "        \"\"\"\n",
    "        turn_card: single tuple (rank, suit)\n",
    "        Fills channel 2 (turn), channel 4 (public), channel 5 (hole+public).\n",
    "        \"\"\"\n",
    "        if turn_card:\n",
    "            r, s = turn_card\n",
    "            self._mark_card(2, r, s)  # Channel 2: turn\n",
    "            self._mark_card(4, r, s)  # Channel 4: public\n",
    "            self._mark_card(5, r, s)  # Channel 5: hole+public\n",
    "            self.public_cards.append(turn_card)\n",
    "    \n",
    "    def set_river(self, river_card):\n",
    "        \"\"\"\n",
    "        river_card: single tuple (rank, suit)\n",
    "        Fills channel 3 (river), channel 4 (public), channel 5 (hole+public).\n",
    "        \"\"\"\n",
    "        if river_card:\n",
    "            r, s = river_card\n",
    "            self._mark_card(3, r, s)  # Channel 3: river\n",
    "            self._mark_card(4, r, s)  # Channel 4: public\n",
    "            self._mark_card(5, r, s)  # Channel 5: hole+public\n",
    "            self.public_cards.append(river_card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Card tensor shape: (6, 4, 13)\n",
      "Card tensor: [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "# 1) Initialize\n",
    "card_rep = CardRepresentation()\n",
    "\n",
    "# 2) Preflop\n",
    "hole_cards = [(12, 3), (12, 2)]  # 'AsAc' (rank=12 => Ace, suits=0 => spade, 2 => diamond)\n",
    "card_rep.set_preflop(hole_cards)\n",
    "\n",
    "# 3) Flop arrives\n",
    "flop = [(7, 1), (3, 3), (9, 2)] # '8d 4s Tc'\n",
    "card_rep.set_flop(flop)\n",
    "\n",
    "# 4) Turn arrives\n",
    "turn = (5, 0) # '6h'\n",
    "card_rep.set_turn(turn)\n",
    "\n",
    "# 5) River arrives\n",
    "river = (11, 3) # 'Qs'\n",
    "card_rep.set_river(river)\n",
    "\n",
    "print(\"Card tensor shape:\", card_rep.card_tensor.shape)  # (6, 4, 13)\n",
    "print(\"Card tensor:\", card_rep.card_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionRepresentation:\n",
    "    \"\"\"\n",
    "    Incrementally build a 24 x 4 x nb action tensor:\n",
    "      - 24 channels => 4 rounds * 6 actions per round\n",
    "      - each channel => shape (4, nb), for [P1 row, P2 row, sum row, legal row] x bet options\n",
    "    \"\"\"\n",
    "    def __init__(self, nb=9, max_actions_per_round=6, rounds=4):\n",
    "        self.nb = nb\n",
    "        self.max_actions = max_actions_per_round\n",
    "        self.rounds = rounds\n",
    "        \n",
    "        # 24 channels total, each is 4 x nb\n",
    "        self.action_tensor = np.zeros((rounds * max_actions_per_round, 4, nb), \n",
    "                                      dtype=np.float32)\n",
    "    \n",
    "    def add_action(self, round_id, action_index_in_round, player_id, action_idx, legal_actions=None, sum_idx=None):\n",
    "        \"\"\"\n",
    "        round_id in [0..3]\n",
    "        action_index_in_round in [0..5]\n",
    "        player_id in [0..1]  (player 1 or player 2)\n",
    "        action_idx in [0..nb-1] (which bet option was chosen)\n",
    "        legal_actions: a list of valid action_idx's at this step (if you want to mark row 3)\n",
    "        sum_idx: optional single int to mark row=2 (the 'sum of bets' row, or pot-size index)\n",
    "        \"\"\"\n",
    "        channel_id = round_id * self.max_actions + action_index_in_round\n",
    "        # Mark the chosen action for the current player\n",
    "        self.action_tensor[channel_id, player_id, action_idx] = 1.0\n",
    "        \n",
    "        # If you want to store sum-of-bets so far in row=2:\n",
    "        if sum_idx is not None and 0 <= sum_idx < self.nb:\n",
    "            self.action_tensor[channel_id, 2, sum_idx] = 1.0\n",
    "        \n",
    "        # If you want to store legal actions in row=3\n",
    "        if legal_actions:\n",
    "            for la in legal_actions:\n",
    "                if 0 <= la < self.nb:\n",
    "                    self.action_tensor[channel_id, 3, la] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action tensor shape: (24, 4, 9)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "action_rep = ActionRepresentation(nb=9, max_actions_per_round=6, rounds=4)\n",
    "\n",
    "# Preflop, first action: round_id=0, action_idx_in_round=0\n",
    "# Player 0 (small blind) \"bet pot\" => let's say pot = 1, action_idx=6 means \"bet pot\"\n",
    "# legal actions might be [0,1,2,3,4,5,6,7,8] if all are valid\n",
    "action_rep.add_action(\n",
    "    round_id=0, \n",
    "    action_index_in_round=0, \n",
    "    player_id=0, \n",
    "    action_idx=6, \n",
    "    legal_actions=range(9),  # all are valid\n",
    "    sum_idx=None            # or some pot-based index if desired\n",
    ")\n",
    "\n",
    "# Next action: round_id=0, action_idx_in_round=1\n",
    "# Player 1 calls => action_idx=1 means \"check/call\"\n",
    "action_rep.add_action(\n",
    "    round_id=0,\n",
    "    action_index_in_round=1,\n",
    "    player_id=1,\n",
    "    action_idx=1,\n",
    "    legal_actions=range(9)\n",
    ")\n",
    "\n",
    "print(\"Action tensor shape:\", action_rep.action_tensor.shape)  # (24, 4, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-Siamese Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PseudoSiameseNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_in_shape=(24, 4, 9),  # Example: (channels=24, H=4, W=9)\n",
    "        card_in_shape=(6, 4, 13),   # Example: (channels=6,  H=4, W=13)\n",
    "        conv_out_dim=128,           # Dim of each branch's embedded output\n",
    "        hidden_dim=256,             # Dim of fused hidden layer\n",
    "        num_actions=9               # Example final policy dimension\n",
    "    ):\n",
    "        super(PseudoSiameseNet, self).__init__()\n",
    "        \n",
    "        # 1) Convolutional branch for the \"action\" tensor\n",
    "        # Example architecture: Conv->Pool->Conv->Flatten->Linear\n",
    "        # (Your architecture may vary; just ensure the output = conv_out_dim)\n",
    "        self.action_conv = nn.Sequential(\n",
    "            nn.Conv2d(action_in_shape[0], 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        # We'll need to figure out the flatten size after these convs.\n",
    "        # A quick trick is to do a test forward pass on dummy data in __init__\n",
    "        \n",
    "        # 2) Convolutional branch for the \"card\" tensor\n",
    "        self.card_conv = nn.Sequential(\n",
    "            nn.Conv2d(card_in_shape[0], 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        # We'll define linear heads to transform each branch's conv output into conv_out_dim\n",
    "        # after we figure out each flatten dimension\n",
    "        dummy_action = torch.zeros(1, action_in_shape[0], action_in_shape[1], action_in_shape[2])\n",
    "        dummy_card   = torch.zeros(1, card_in_shape[0],   card_in_shape[1],   card_in_shape[2])\n",
    "        \n",
    "        # Pass through each conv to see resulting shape\n",
    "        with torch.no_grad():\n",
    "            act_out = self.action_conv(dummy_action)\n",
    "            card_out = self.card_conv(dummy_card)\n",
    "            # Flatten dimension\n",
    "            self.act_conv_flat_size  = act_out.view(1, -1).size(1)\n",
    "            self.card_conv_flat_size = card_out.view(1, -1).size(1)\n",
    "        \n",
    "        # Now define linear layers to get each branch to conv_out_dim\n",
    "        self.action_fc = nn.Sequential(\n",
    "            nn.Linear(self.act_conv_flat_size, conv_out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.card_fc = nn.Sequential(\n",
    "            nn.Linear(self.card_conv_flat_size, conv_out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 3) Fusion FC layers\n",
    "        # After concatenation, total input dim = conv_out_dim * 2\n",
    "        fusion_in_dim = conv_out_dim * 2\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(fusion_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 4) Output heads: policy and value\n",
    "        self.policy_head = nn.Linear(hidden_dim, num_actions)\n",
    "        self.value_head  = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, action_input, card_input):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            action_input: shape (B, 24, 4, 9) for example\n",
    "            card_input:   shape (B, 6, 4, 13)\n",
    "        Outputs:\n",
    "            policy_logits: shape (B, num_actions)\n",
    "            value:         shape (B, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # ----- Branch A: Action Info -----\n",
    "        x_a = self.action_conv(action_input)     # shape: (B, 64, H', W')\n",
    "        x_a = x_a.view(x_a.size(0), -1)          # flatten\n",
    "        x_a = self.action_fc(x_a)                # shape: (B, conv_out_dim)\n",
    "        \n",
    "        # ----- Branch B: Card Info -----\n",
    "        x_c = self.card_conv(card_input)         # shape: (B, 64, H'', W'')\n",
    "        x_c = x_c.view(x_c.size(0), -1)\n",
    "        x_c = self.card_fc(x_c)                  # shape: (B, conv_out_dim)\n",
    "        \n",
    "        # ----- Fuse -----\n",
    "        x = torch.cat([x_a, x_c], dim=1)         # shape: (B, 2*conv_out_dim)\n",
    "        x = self.fusion_fc(x)                    # shape: (B, hidden_dim)\n",
    "        \n",
    "        # ----- Heads -----\n",
    "        policy_logits = self.policy_head(x)      # shape: (B, num_actions)\n",
    "        value         = self.value_head(x)       # shape: (B, 1)\n",
    "        \n",
    "        return policy_logits, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_probs(logits):\n",
    "    \"\"\"\n",
    "    Convert raw policy logits to probabilities.\n",
    "    This is a common softmax pattern.\n",
    "    \"\"\"\n",
    "    return F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_logits shape: torch.Size([1, 9])\n",
      "value shape: torch.Size([1, 1])\n",
      "tensor([[-0.0311,  0.0471, -0.0182,  0.0107,  0.0620,  0.0195,  0.0459, -0.0301,\n",
      "          0.0069]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0322]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Suppose batch_size=2 for quick test\n",
    "batch_size = 1\n",
    "\n",
    "# Create random input for action tensor: (B, 24, 4, 9)\n",
    "action_input = torch.randn(batch_size, 24, 4, 9)\n",
    "# Create random input for card tensor: (B, 6, 4, 13)\n",
    "card_input = torch.randn(batch_size, 6, 4, 13)\n",
    "\n",
    "model = PseudoSiameseNet(\n",
    "    action_in_shape=(24, 4, 9),\n",
    "    card_in_shape=(6, 4, 13),\n",
    "    conv_out_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_actions=9\n",
    ")\n",
    "\n",
    "policy_logits, value = model(action_input, card_input)\n",
    "print(\"policy_logits shape:\", policy_logits.shape)  # (B, 9)\n",
    "print(\"value shape:\", value.shape)                  # (B, 1)\n",
    "\n",
    "print(policy_logits)\n",
    "print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
