{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from ai.debug_utils import debug_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_gamma(rewards: np.array, gamma):\n",
    "    r_gamma = 0\n",
    "    for reward in rewards[:0:-1]:  # Slicing to reverse except the first element\n",
    "        r_gamma = gamma * (r_gamma + reward)\n",
    "        debug_print(\"reward: \", reward)\n",
    "    r_gamma += rewards[0]\n",
    "    return r_gamma\n",
    "\n",
    "def value_function(state): #TODO: implement this\n",
    "    return 0\n",
    "\n",
    "def v_loss(r_gamma, state, deltas):\n",
    "    return (np.clip(r_gamma, -deltas[1], deltas[2])-value_function(state))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    if state == 'Preflop':\n",
    "        # return np.array([0.5, 0.5, 0, 0, 0])\n",
    "        return np.array([0, 1, 0, 0, 0])\n",
    "    elif state == 'Flop':\n",
    "        # return np.array([0, 0.5, 0.5, 0, 0])\n",
    "        return np.array([0, 0, 1, 0, 0])\n",
    "    elif state == 'Turn':\n",
    "        # return np.array([0, 0, 1/3, 1/3, 1/3])\n",
    "        return np.array([0, 0, 0, 1, 0])\n",
    "    elif state == 'River':\n",
    "        # return np.array([0, 1/2, 1/2, 0, 0])\n",
    "        return np.array([0, 0, 1, 0, 0])\n",
    "    return 0\n",
    "\n",
    "def get_deltas(state): # I'm not sure wether the deltas should add the chips from previous states or just the current state\n",
    "    delta1 = 3\n",
    "    if state == 'Preflop':\n",
    "        delta2 = 20\n",
    "        delta3 = 10 # The opponent put in the big blind and the agent just betted. The opponnent hasnt put in any chips yet.\n",
    "    elif state == 'Flop':\n",
    "        delta2 = 40\n",
    "        delta3 = 20\n",
    "    elif state == 'Turn':\n",
    "        delta2 = 120\n",
    "        delta3 = 80\n",
    "    elif state == 'River':\n",
    "        delta2 = 120\n",
    "        delta3 = 120\n",
    "    return delta1, delta2, delta3\n",
    "\n",
    "def ratio(old_policy, new_policy, action, state):\n",
    "    return new_policy(state)[action] / old_policy(state)[action]\n",
    "\n",
    "def a_gae(results, states, value_function, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Generalized Advantage Estimator (GAE) where:\n",
    "      - len(states) == len(results)\n",
    "      - We do NOT assume an extra 'terminal state' beyond these states.\n",
    "    \n",
    "    results:       list/array of rewards at each timestep\n",
    "    states:        list/array of states at each timestep\n",
    "    value_function: function that takes a state and returns a scalar value\n",
    "    gamma:         discount factor\n",
    "    lambda_:       GAE parameter\n",
    "    \"\"\"\n",
    "    N = len(results)\n",
    "    if N == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # For convenience, compute V(s0) once\n",
    "    v0 = value_function(states[0])\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # 1) Precompute partial sums of discounted rewards:\n",
    "    #    S[k] = sum_{i=0..k-1} gamma^i * results[i], with S[0] = 0\n",
    "    #\n",
    "    #    Then the \"raw\" advantage term (before weighting by λ^(k-1)) is:\n",
    "    #       a_k = - V(s0) + S[k] + gamma^k * V(sk),\n",
    "    #    for k in 1..N-1 (because states[k] must be valid).\n",
    "    # --------------------------------------------------------\n",
    "    S = np.zeros(N+1, dtype=float)\n",
    "    for i in range(N):\n",
    "        S[i+1] = S[i] + (gamma ** i) * results[i]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2) Accumulate the GAE sum:\n",
    "    #\n",
    "    #    A = (1 - λ) * Σ (λ^(k-1) * a_k), for k = 1..N-1\n",
    "    #\n",
    "    #    We use k=1..N-1 so that states[k] is still in range.\n",
    "    # --------------------------------------------------------\n",
    "    gae_sum = 0.0\n",
    "    for k in range(1, N):\n",
    "        a_k = -v0 + S[k] + (gamma ** k) * value_function(states[k])\n",
    "        gae_sum += (lambda_ ** (k - 1)) * a_k\n",
    "    \n",
    "    return (1 - lambda_) * gae_sum\n",
    "\n",
    "# I wasn't sure how to treat the showdown state. The approach I am following is when the only states that are fed to the a_gae function are the river and the showdown, the resulting a_k() is \n",
    "# -V(river) + r(river) + V(showdown). I think the values for the river state and the showdown state will be different because the showdown value depends on the amount of chips that the agent has played\n",
    "# in the river state.\n",
    "    \n",
    "def tc_loss_function(ratio, advantage, epsilon, deltas): #We compute this for every hand and then average it\n",
    "    return np.clip(ratio, np.clip(ratio, 1 - epsilon, 1 + epsilon), deltas[0]) * advantage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(policy: callable, state):\n",
    "    return np.random.choice(len(policy(state)), p=policy(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward:  320\n",
      "reward:  -100\n",
      "reward:  0\n",
      "reward:  -40\n",
      "159.06161882032\n",
      "reward:  320\n",
      "reward:  -100\n",
      "reward:  0\n",
      "reward:  -40\n",
      "25300.598581740778\n"
     ]
    }
   ],
   "source": [
    "rewards = np.array([-20, -40, 0, -100, 320])\n",
    "deltas = [3, 160, 160]\n",
    "debug_print(r_gamma(rewards, 0.999))\n",
    "debug_print(v_loss(r_gamma(rewards, 0.999), 0, deltas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(states, rewards, policy, value_function):\n",
    "    tc_loss = 0\n",
    "    value_loss = 0\n",
    "    states_without_showdown = states[:-1]\n",
    "    for i, state in enumerate(states_without_showdown):\n",
    "        deltas = get_deltas(state)\n",
    "        rewards_from_now = rewards[i:]\n",
    "        states_from_now = states[i:]\n",
    "        advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99) #I'm not sure if this is correct\n",
    "        action = get_action(policy, state)\n",
    "        old_policy = policy\n",
    "        new_policy = policy\n",
    "        r = ratio(old_policy, new_policy, action, state)\n",
    "        tc_loss += tc_loss_function(r, advantage, 0.2, deltas)\n",
    "        value_loss += v_loss(r_gamma(rewards_from_now, 0.999), 0, deltas)\n",
    "\n",
    "    tc_loss /= len(states_without_showdown)\n",
    "    value_loss /= len(states_without_showdown)\n",
    "    return tc_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
      "deltas:  (3, 20, 10)\n",
      "Advantage:  -2.932771642119203\n",
      "20 10\n",
      "reward:  240\n",
      "reward:  0\n",
      "reward:  -80\n",
      "reward:  -20\n",
      "['Flop', 'Turn', 'River', 'Showdown']\n",
      "deltas:  (3, 40, 20)\n",
      "Advantage:  -2.168523920000002\n",
      "40 20\n",
      "reward:  240\n",
      "reward:  0\n",
      "reward:  -80\n",
      "['Turn', 'River', 'Showdown']\n",
      "deltas:  (3, 120, 80)\n",
      "Advantage:  -1.5920000000000012\n",
      "120 80\n",
      "reward:  240\n",
      "reward:  0\n",
      "['River', 'Showdown']\n",
      "deltas:  (3, 120, 120)\n",
      "Advantage:  0.0\n",
      "120 120\n",
      "reward:  240\n",
      "TC loss:  -1.6733238905298016\n",
      "Value loss:  5325.0\n"
     ]
    }
   ],
   "source": [
    "states = ['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
    "rewards = [-20, -20, -80, 0, 240] #There should be one more reward than states\n",
    "tc_loss = 0\n",
    "value_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "states_without_showdown = states[:-1]\n",
    "for i, state in enumerate(states_without_showdown):\n",
    "    deltas = get_deltas(state)\n",
    "    debug_print(states[i:])\n",
    "    debug_print('deltas: ', deltas)\n",
    "    rewards_from_now = rewards[i:]\n",
    "    states_from_now = states[i:]\n",
    "    advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99) #I'm not sure if this is correct\n",
    "    debug_print('Advantage: ', advantage)\n",
    "    action = get_action(policy, state)\n",
    "    old_policy = policy\n",
    "    new_policy = policy\n",
    "    r = ratio(old_policy, new_policy, action, state)\n",
    "    debug_print(deltas[1], deltas[2])\n",
    "    tc_loss += tc_loss_function(r, advantage, 0.2, deltas)\n",
    "    value_loss += v_loss(r_gamma(rewards_from_now, 0.999), 0, deltas)\n",
    "\n",
    "tc_loss /= len(states_without_showdown)\n",
    "value_loss /= len(states_without_showdown)\n",
    "debug_print('TC loss: ', tc_loss)\n",
    "debug_print('Value loss: ', value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
      "Advantage:  -2.932771642119203\n",
      "20 10\n",
      "240\n",
      "0\n",
      "-80\n",
      "-20\n",
      "['Flop', 'Turn', 'River', 'Showdown']\n",
      "Advantage:  -2.168523920000002\n",
      "40 20\n",
      "240\n",
      "0\n",
      "-80\n",
      "['Turn', 'River', 'Showdown']\n",
      "Advantage:  -1.5920000000000012\n",
      "120 80\n",
      "240\n",
      "0\n",
      "['River', 'Showdown']\n",
      "Advantage:  0.0\n",
      "120 120\n",
      "240\n",
      "TC loss:  -1.6733238905298016\n",
      "Value loss:  5325.0\n"
     ]
    }
   ],
   "source": [
    "states = ['Preflop', 'Flop', 'Turn', 'River', 'Showdown']\n",
    "rewards = [-20, -20, -80, 0, 240] #There should be one more reward than states\n",
    "tc_loss = 0\n",
    "value_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "states_without_showdown = states[:-1]\n",
    "for i, state in enumerate(states_without_showdown):\n",
    "    deltas = get_deltas(state)\n",
    "    debug_print(states[i:])\n",
    "    rewards_from_now = rewards[i:]\n",
    "    states_from_now = states[i:]\n",
    "    advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99) #I'm not sure if this is correct\n",
    "    debug_print('Advantage: ', advantage)\n",
    "    action = get_action(policy, state)\n",
    "    old_policy = policy\n",
    "    new_policy = policy\n",
    "    r = ratio(old_policy, new_policy, action, state)\n",
    "    debug_print(deltas[1], deltas[2])\n",
    "    tc_loss += tc_loss_function(r, advantage, 0.2, deltas)\n",
    "    value_loss += v_loss(r_gamma(rewards_from_now, 0.999), 0, deltas)\n",
    "\n",
    "tc_loss /= len(states_without_showdown)\n",
    "value_loss /= len(states_without_showdown)\n",
    "debug_print('TC loss: ', tc_loss)\n",
    "debug_print('Value loss: ', value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [-20, -20, -80, 0, 240]\n",
      "2 [-20, -20, -80, 0, 240]\n",
      "3 [-20, -20, -80, 0, 240]\n",
      "4 [-20, -20, -80, 0, 240]\n",
      "240\n",
      "0\n",
      "-80\n",
      "-20\n",
      "1 [-20, -80, 0, 240]\n",
      "2 [-20, -80, 0, 240]\n",
      "3 [-20, -80, 0, 240]\n",
      "240\n",
      "0\n",
      "-80\n",
      "1 [-80, 0, 240]\n",
      "2 [-80, 0, 240]\n",
      "240\n",
      "0\n",
      "1 [0, 240]\n",
      "240\n",
      "(np.float64(-0.9304432345098008), np.float64(5325.0))\n"
     ]
    }
   ],
   "source": [
    "debug_print(get_losses(states, rewards, policy, value_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game State Representation\n",
    "\n",
    "## Card representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CardRepresentation:\n",
    "    \"\"\"\n",
    "    Incrementally build a 6 x 4 x 13 card tensor:\n",
    "      - Channel 0: hole cards\n",
    "      - Channel 1: flop\n",
    "      - Channel 2: turn\n",
    "      - Channel 3: river\n",
    "      - Channel 4: all public (flop+turn+river)\n",
    "      - Channel 5: hole + public\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Create it once, fill incrementally\n",
    "        self.card_tensor = np.zeros((6, 4, 13), dtype=np.float32)\n",
    "        \n",
    "        # Track which cards have been seen\n",
    "        self.hole_cards = []\n",
    "        self.public_cards = []\n",
    "    \n",
    "    def _mark_card(self, channel, rank, suit):\n",
    "        \"\"\"Helper to set a 1 for (channel, suit, rank).\"\"\"\n",
    "        self.card_tensor[channel, suit, rank] = 1.0\n",
    "    \n",
    "    def set_preflop(self, hole_cards):\n",
    "        \"\"\"\n",
    "        hole_cards: list of 2 tuples [(rank, suit), (rank, suit)]\n",
    "        Fills channel 0 (hole) and partially updates channel 5 (hole+public).\n",
    "        \"\"\"\n",
    "        self.hole_cards = hole_cards[:]  # store\n",
    "        for (r, s) in hole_cards:\n",
    "            self._mark_card(0, r, s)  # Channel 0: hole cards\n",
    "            self._mark_card(5, r, s)  # Channel 5: hole+public (so far, just hole)\n",
    "    \n",
    "    def set_flop(self, flop_cards):\n",
    "        \"\"\"\n",
    "        flop_cards: list of 3 tuples [(rank, suit), ...]\n",
    "        Fills channel 1 (flop), channel 4 (all public), channel 5 (hole+public).\n",
    "        \"\"\"\n",
    "        for (r, s) in flop_cards:\n",
    "            self._mark_card(1, r, s)  # Channel 1: flop\n",
    "            self._mark_card(4, r, s)  # Channel 4: public\n",
    "            self._mark_card(5, r, s)  # Channel 5: hole+public\n",
    "        self.public_cards.extend(flop_cards)\n",
    "    \n",
    "    def set_turn(self, turn_card):\n",
    "        \"\"\"\n",
    "        turn_card: single tuple (rank, suit)\n",
    "        Fills channel 2 (turn), channel 4 (public), channel 5 (hole+public).\n",
    "        \"\"\"\n",
    "        if turn_card:\n",
    "            r, s = turn_card\n",
    "            self._mark_card(2, r, s)  # Channel 2: turn\n",
    "            self._mark_card(4, r, s)  # Channel 4: public\n",
    "            self._mark_card(5, r, s)  # Channel 5: hole+public\n",
    "            self.public_cards.append(turn_card)\n",
    "    \n",
    "    def set_river(self, river_card):\n",
    "        \"\"\"\n",
    "        river_card: single tuple (rank, suit)\n",
    "        Fills channel 3 (river), channel 4 (public), channel 5 (hole+public).\n",
    "        \"\"\"\n",
    "        if river_card:\n",
    "            r, s = river_card\n",
    "            self._mark_card(3, r, s)  # Channel 3: river\n",
    "            self._mark_card(4, r, s)  # Channel 4: public\n",
    "            self._mark_card(5, r, s)  # Channel 5: hole+public\n",
    "            self.public_cards.append(river_card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Card tensor shape: (6, 4, 13)\n",
      "Card tensor: [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "# 1) Initialize\n",
    "card_rep = CardRepresentation()\n",
    "\n",
    "# 2) Preflop\n",
    "hole_cards = [(12, 3), (12, 2)]  # 'AsAc' (rank=12 => Ace, suits=0 => spade, 2 => diamond)\n",
    "card_rep.set_preflop(hole_cards)\n",
    "\n",
    "# 3) Flop arrives\n",
    "flop = [(7, 1), (3, 3), (9, 2)] # '8d 4s Tc'\n",
    "card_rep.set_flop(flop)\n",
    "\n",
    "# 4) Turn arrives\n",
    "turn = (5, 0) # '6h'\n",
    "card_rep.set_turn(turn)\n",
    "\n",
    "# 5) River arrives\n",
    "river = (11, 3) # 'Qs'\n",
    "card_rep.set_river(river)\n",
    "\n",
    "debug_print(\"Card tensor shape:\", card_rep.card_tensor.shape)  # (6, 4, 13)\n",
    "debug_print(\"Card tensor:\", card_rep.card_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You build a (24, 4, 9) tensor in the example (rounds=4, max_actions_per_round=6, nb=9):\n",
    "So there are 24 “channels” → (4 betting rounds × 6 possible actions per round).\n",
    "Each channel is a 4×9 “mini-grid,” where:\n",
    "row=0/1 might mark which action was chosen by player 0 or 1,\n",
    "row=2 might store cumulative pot or sum,\n",
    "row=3 might store legality of each action.\n",
    "The idea is that you “fill in” these channels incrementally as the hand progresses (and each action is taken). Then the CNN can glean the pattern and sequence of actions so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionRepresentation:\n",
    "    \"\"\"\n",
    "    Incrementally build a 24 x 4 x nb action tensor:\n",
    "      - 24 channels => 4 rounds * 6 actions per round\n",
    "      - each channel => shape (4, nb), for [P1 row, P2 row, sum row, legal row] x bet options\n",
    "    \"\"\"\n",
    "    def __init__(self, nb=9, max_actions_per_round=6, rounds=4):\n",
    "        self.nb = nb\n",
    "        self.max_actions = max_actions_per_round\n",
    "        self.rounds = rounds\n",
    "        \n",
    "        # 24 channels total, each is 4 x nb (in this case 4x9)\n",
    "        self.action_tensor = np.zeros((rounds * max_actions_per_round, 4, nb), \n",
    "                                      dtype=np.float32)\n",
    "    \n",
    "    def add_action(self, round_id, action_index_in_round, player_id, action_idx, legal_actions=None, sum_idx=None):\n",
    "        \"\"\"\n",
    "        round_id in [0..3]\n",
    "        action_index_in_round in [0..5]\n",
    "        player_id in [0..1]  (player 1 or player 2)\n",
    "        action_idx in [0..nb-1] (which bet option was chosen)\n",
    "        legal_actions: a list of valid action_idx's at this step (if you want to mark row 3)\n",
    "        sum_idx: optional single int to mark row=2 (the 'sum of bets' row, or pot-size index)\n",
    "        \"\"\"\n",
    "        channel_id = round_id * self.max_actions + action_index_in_round\n",
    "        # Mark the chosen action for the current player\n",
    "        self.action_tensor[channel_id, player_id, action_idx] = 1.0\n",
    "        \n",
    "        # If you want to store sum-of-bets so far in row=2:\n",
    "        if sum_idx is not None and 0 <= sum_idx < self.nb:\n",
    "            self.action_tensor[channel_id, 2, sum_idx] = 1.0\n",
    "        \n",
    "        # If you want to store legal actions in row=3\n",
    "        if legal_actions:\n",
    "            for la in legal_actions:\n",
    "                if 0 <= la < self.nb:\n",
    "                    self.action_tensor[channel_id, 3, la] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action tensor shape: (24, 4, 9)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "action_rep = ActionRepresentation(nb=9, max_actions_per_round=6, rounds=4)\n",
    "\n",
    "# Preflop, first action: round_id=0, action_idx_in_round=0\n",
    "# Player 0 (small blind) \"bet pot\" => let's say pot = 1, action_idx=6 means \"bet pot\"\n",
    "# legal actions might be [0,1,2,3,4,5,6,7,8] if all are valid\n",
    "action_rep.add_action(\n",
    "    round_id=0, \n",
    "    action_index_in_round=0, \n",
    "    player_id=0, \n",
    "    action_idx=6, \n",
    "    legal_actions=range(9),  # all are valid (actually here only calling, raising or folding or available since we need to match BB)\n",
    "    sum_idx=None            # or some pot-based index if desired\n",
    ")\n",
    "\n",
    "# Next action: round_id=0, action_idx_in_round=1\n",
    "# Player 1 calls => action_idx=1 means \"check/call\"\n",
    "action_rep.add_action(\n",
    "    round_id=0,\n",
    "    action_index_in_round=1,\n",
    "    player_id=1,\n",
    "    action_idx=1,\n",
    "    legal_actions=range(9)\n",
    ")\n",
    "\n",
    "debug_print(\"Action tensor shape:\", action_rep.action_tensor.shape)  # (24, 4, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-Siamese Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PseudoSiameseNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_in_shape=(24, 4, 9),  # Example: (channels=24, H=4, W=9)\n",
    "        card_in_shape=(6, 4, 13),   # Example: (channels=6,  H=4, W=13)\n",
    "        conv_out_dim=128,           # Dim of each branch's embedded output\n",
    "        hidden_dim=256,             # Dim of fused hidden layer\n",
    "        num_actions=9               # Example final policy dimension\n",
    "    ):\n",
    "        super(PseudoSiameseNet, self).__init__()\n",
    "        \n",
    "        # 1) Convolutional branch for the \"action\" tensor\n",
    "        # Example architecture: Conv->Pool->Conv->Flatten->Linear\n",
    "        # (Your architecture may vary; just ensure the output = conv_out_dim)\n",
    "        self.action_conv = nn.Sequential(\n",
    "            nn.Conv2d(action_in_shape[0], 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        # We'll need to figure out the flatten size after these convs.\n",
    "        # A quick trick is to do a test forward pass on dummy data in __init__\n",
    "        \n",
    "        # 2) Convolutional branch for the \"card\" tensor\n",
    "        self.card_conv = nn.Sequential(\n",
    "            nn.Conv2d(card_in_shape[0], 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        # We'll define linear heads to transform each branch's conv output into conv_out_dim\n",
    "        # after we figure out each flatten dimension\n",
    "        dummy_action = torch.zeros(1, action_in_shape[0], action_in_shape[1], action_in_shape[2])\n",
    "        dummy_card   = torch.zeros(1, card_in_shape[0],   card_in_shape[1],   card_in_shape[2])\n",
    "        \n",
    "        # Pass through each conv to see resulting shape\n",
    "        with torch.no_grad():\n",
    "            act_out = self.action_conv(dummy_action)\n",
    "            card_out = self.card_conv(dummy_card)\n",
    "            # Flatten dimension\n",
    "            self.act_conv_flat_size  = act_out.view(1, -1).size(1)\n",
    "            self.card_conv_flat_size = card_out.view(1, -1).size(1)\n",
    "        \n",
    "        # Now define linear layers to get each branch to conv_out_dim\n",
    "        self.action_fc = nn.Sequential(\n",
    "            nn.Linear(self.act_conv_flat_size, conv_out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.card_fc = nn.Sequential(\n",
    "            nn.Linear(self.card_conv_flat_size, conv_out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 3) Fusion FC layers\n",
    "        # After concatenation, total input dim = conv_out_dim * 2\n",
    "        fusion_in_dim = conv_out_dim * 2\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(fusion_in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 4) Output heads: policy and value\n",
    "        self.policy_head = nn.Linear(hidden_dim, num_actions)\n",
    "        self.value_head  = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, action_input, card_input):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            action_input: shape (B, 24, 4, 9) for example\n",
    "            card_input:   shape (B, 6, 4, 13)\n",
    "        Outputs:\n",
    "            policy_logits: shape (B, num_actions)\n",
    "            value:         shape (B, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        # ----- Branch A: Action Info -----\n",
    "        x_a = self.action_conv(action_input)     # shape: (B, 64, H', W')\n",
    "        x_a = x_a.view(x_a.size(0), -1)          # flatten\n",
    "        x_a = self.action_fc(x_a)                # shape: (B, conv_out_dim)\n",
    "        \n",
    "        # ----- Branch B: Card Info -----\n",
    "        x_c = self.card_conv(card_input)         # shape: (B, 64, H'', W'')\n",
    "        x_c = x_c.view(x_c.size(0), -1)\n",
    "        x_c = self.card_fc(x_c)                  # shape: (B, conv_out_dim)\n",
    "        \n",
    "        # ----- Fuse -----\n",
    "        x = torch.cat([x_a, x_c], dim=1)         # shape: (B, 2*conv_out_dim)\n",
    "        x = self.fusion_fc(x)                    # shape: (B, hidden_dim)\n",
    "        \n",
    "        # ----- Heads -----\n",
    "        policy_logits = self.policy_head(x)      # shape: (B, num_actions)\n",
    "        value         = self.value_head(x)       # shape: (B, 1)\n",
    "        \n",
    "        return policy_logits, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_probs(logits):\n",
    "    \"\"\"\n",
    "    Convert raw policy logits to probabilities.\n",
    "    This is a common softmax pattern.\n",
    "    \"\"\"\n",
    "    return F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_logits shape: torch.Size([1, 9])\n",
      "value shape: torch.Size([1, 1])\n",
      "tensor([[-0.0289,  0.0285,  0.0359, -0.0546, -0.0115, -0.0646, -0.0314,  0.0004,\n",
      "          0.0022]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0673]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Suppose batch_size=2 for quick test\n",
    "batch_size = 1\n",
    "\n",
    "# Create random input for action tensor: (B, 24, 4, 9)\n",
    "action_input = torch.randn(batch_size, 24, 4, 9)\n",
    "# Create random input for card tensor: (B, 6, 4, 13)\n",
    "card_input = torch.randn(batch_size, 6, 4, 13)\n",
    "\n",
    "model = PseudoSiameseNet(\n",
    "    action_in_shape=(24, 4, 9),\n",
    "    card_in_shape=(6, 4, 13),\n",
    "    conv_out_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_actions=9\n",
    ")\n",
    "\n",
    "policy_logits, value = model(action_input, card_input)\n",
    "debug_print(\"policy_logits shape:\", policy_logits.shape)  # (B, 9)\n",
    "debug_print(\"value shape:\", value.shape)                  # (B, 1)\n",
    "\n",
    "debug_print(policy_logits)\n",
    "debug_print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_from_probs(probs):\n",
    "    \"\"\"Sample an action index given a numpy array of probabilities.\"\"\"\n",
    "    return np.random.choice(len(probs), p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ITERATION 1 ...\n",
      "\n",
      "=== Iteration 1 ===\n",
      "reward:  240\n",
      "reward:  0\n",
      "reward:  -80\n",
      "reward:  -20\n",
      "  State=Preflop, advantage=-2.93, chosen_action=2\n",
      "    distribution=[0.11144958 0.11221532 0.10817553 0.10764666 0.11327564 0.11123017\n",
      " 0.11398275 0.11353203 0.10849233]\n",
      "    pol_loss_val=-2.933, val_loss_val=100.000\n",
      "reward:  240\n",
      "reward:  0\n",
      "reward:  -80\n",
      "  State=Flop, advantage=-2.17, chosen_action=7\n",
      "    distribution=[0.11164989 0.11254499 0.10774592 0.1077005  0.113539   0.11010551\n",
      " 0.11415704 0.11402137 0.10853586]\n",
      "    pol_loss_val=-2.169, val_loss_val=400.000\n",
      "reward:  240\n",
      "reward:  0\n",
      "  State=Turn, advantage=-1.59, chosen_action=7\n",
      "    distribution=[0.11223447 0.11286046 0.10725048 0.107858   0.11389577 0.10940454\n",
      " 0.11449676 0.11322542 0.10877414]\n",
      "    pol_loss_val=-1.592, val_loss_val=6400.000\n",
      "reward:  240\n",
      "  State=River, advantage=0.00, chosen_action=5\n",
      "    distribution=[0.11293585 0.1133142  0.10618255 0.10805468 0.11445083 0.10903878\n",
      " 0.11470548 0.11203111 0.10928655]\n",
      "    pol_loss_val=0.000, val_loss_val=14400.000\n",
      "\n",
      "Actions chosen this iteration: [2, 7, 7, 5]\n",
      "Policy distributions each step:\n",
      "  [0.111 0.112 0.108 0.108 0.113 0.111 0.114 0.114 0.108]\n",
      "  [0.112 0.113 0.108 0.108 0.114 0.11  0.114 0.114 0.109]\n",
      "  [0.112 0.113 0.107 0.108 0.114 0.109 0.114 0.113 0.109]\n",
      "  [0.113 0.113 0.106 0.108 0.114 0.109 0.115 0.112 0.109]\n",
      "\n",
      "=> iteration 1 done. avg policy loss=-1.673, avg value loss=5325.000\n",
      "\n",
      "RUNNING ITERATION 2 (same scenario) ...\n",
      "\n",
      "=== Iteration 2 ===\n",
      "reward:  240\n",
      "reward:  0\n",
      "reward:  -80\n",
      "reward:  -20\n",
      "  State=Preflop, advantage=-2.93, chosen_action=8\n",
      "    distribution=[0.11351511 0.11414205 0.10498959 0.10864417 0.11511807 0.10847928\n",
      " 0.11459956 0.11058684 0.10992531]\n",
      "    pol_loss_val=-2.933, val_loss_val=100.000\n",
      "reward:  240\n",
      "reward:  0\n",
      "reward:  -80\n",
      "  State=Flop, advantage=-2.17, chosen_action=0\n",
      "    distribution=[0.11416431 0.11506072 0.10384353 0.10937791 0.11594762 0.10821812\n",
      " 0.11472853 0.10910437 0.10955495]\n",
      "    pol_loss_val=-2.169, val_loss_val=400.000\n",
      "reward:  240\n",
      "reward:  0\n",
      "  State=Turn, advantage=-1.59, chosen_action=0\n",
      "    distribution=[0.11386036 0.11623941 0.10256689 0.11029692 0.11721964 0.1081586\n",
      " 0.1150885  0.10761495 0.10895472]\n",
      "    pol_loss_val=-1.592, val_loss_val=6400.000\n",
      "reward:  240\n",
      "  State=River, advantage=0.00, chosen_action=5\n",
      "    distribution=[0.11306424 0.11822988 0.10079886 0.1112795  0.11913526 0.10807182\n",
      " 0.11578117 0.10556974 0.10806957]\n",
      "    pol_loss_val=0.000, val_loss_val=14400.000\n",
      "\n",
      "Actions chosen this iteration: [8, 0, 0, 5]\n",
      "Policy distributions each step:\n",
      "  [0.114 0.114 0.105 0.109 0.115 0.108 0.115 0.111 0.11 ]\n",
      "  [0.114 0.115 0.104 0.109 0.116 0.108 0.115 0.109 0.11 ]\n",
      "  [0.114 0.116 0.103 0.11  0.117 0.108 0.115 0.108 0.109]\n",
      "  [0.113 0.118 0.101 0.111 0.119 0.108 0.116 0.106 0.108]\n",
      "\n",
      "=> iteration 2 done. avg policy loss=-1.673, avg value loss=5325.000\n",
      "\n",
      "COMPARISON:\n",
      " Iteration 1 chosen actions: [2, 7, 7, 5]\n",
      " Iteration 2 chosen actions: [8, 0, 0, 5]\n",
      "\n",
      "You may see differences in the chosen actions or in the distributions,\n",
      "which shows the policy was updated between iterations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "##########################################################\n",
    "# 4) DEMONSTRATION: TWO ITERATIONS WITH POLICY CHANGES\n",
    "##########################################################\n",
    "\n",
    "def build_card_rep():\n",
    "    \"\"\"Build the same card scenario each time.\"\"\"\n",
    "    cr = CardRepresentation()\n",
    "    # Preflop: As,Ac\n",
    "    cr.set_preflop([(12,3),(12,2)])\n",
    "    # Flop: 8d,4s,Tc\n",
    "    cr.set_flop([(7,1),(3,3),(9,2)])\n",
    "    # Turn: 6h\n",
    "    cr.set_turn((5,0))\n",
    "    # River: Qs\n",
    "    cr.set_river((11,3))\n",
    "    return cr\n",
    "\n",
    "def build_action_rep():\n",
    "    \"\"\"Initial action rep with 2 actions on Preflop for demonstration.\"\"\"\n",
    "    ar = ActionRepresentation(nb=9, max_actions_per_round=6, rounds=4)\n",
    "    # Preflop action 0 => channel 0\n",
    "    ar.add_action(0, 0, 0, 6, legal_actions=range(9))\n",
    "    # Preflop action 1 => channel 1\n",
    "    ar.add_action(0, 1, 1, 1, legal_actions=range(9))\n",
    "    return ar\n",
    "\n",
    "def to_torch_input(card_rep, action_rep):\n",
    "    \"\"\"Convert to shape (1,...) for model.\"\"\"\n",
    "    card_np = card_rep.card_tensor[np.newaxis,...]\n",
    "    action_np = action_rep.action_tensor[np.newaxis,...]\n",
    "    return torch.from_numpy(action_np).float(), torch.from_numpy(card_np).float()\n",
    "\n",
    "def run_one_iteration(model, optimizer, iteration_idx):\n",
    "    \"\"\"\n",
    "    Runs the SAME scenario: states=['Preflop','Flop','Turn','River','Showdown'],\n",
    "    rewards=[-20, -20, -80, 0, 240].\n",
    "    We'll do a single pass: compute a policy distribution from the model,\n",
    "    pick an action, do the advantage, policy loss, value loss, THEN update the model.\n",
    "\n",
    "    Because we do an update, the next iteration might yield different policy distributions\n",
    "    => different actions.\n",
    "    \"\"\"\n",
    "    debug_print(f\"\\n=== Iteration {iteration_idx} ===\")\n",
    "    # Same states & rewards\n",
    "    states = ['Preflop','Flop','Turn','River','Showdown']\n",
    "    rewards = [-20, -20, -80, 0, 240]\n",
    "    \n",
    "    # Build reps\n",
    "    card_rep = build_card_rep()\n",
    "    action_rep = build_action_rep()\n",
    "    \n",
    "    # We'll track total policy loss and value loss\n",
    "    tc_loss_total = 0\n",
    "    value_loss_total = 0\n",
    "    steps_count = 0\n",
    "    \n",
    "    # We'll store the final distribution & chosen action\n",
    "    chosen_actions = []\n",
    "    policy_distributions = []\n",
    "    \n",
    "    # (like your code, only up to the 2nd-last state)\n",
    "    for i, state in enumerate(states[:-1]):\n",
    "        # A) GAE advantage\n",
    "        rewards_from_now = rewards[i:]\n",
    "        states_from_now  = states[i:]\n",
    "        advantage = a_gae(rewards_from_now, states_from_now, value_function, 0.999, 0.99)\n",
    "        \n",
    "        # B) Model forward => get policy distribution\n",
    "        action_t, card_t = to_torch_input(card_rep, action_rep)\n",
    "        policy_logits, val_out = model(action_t, card_t)\n",
    "        probs = logits_to_probs(policy_logits)[0].detach().numpy()  # shape (9,)\n",
    "        \n",
    "        # pick an action from this distribution\n",
    "        action_idx = get_action_from_probs(probs)\n",
    "        \n",
    "        # store distribution + chosen action\n",
    "        policy_distributions.append(probs)\n",
    "        chosen_actions.append(action_idx)\n",
    "        \n",
    "        # C) ratio = 1 for demonstration (we're not storing old vs. new)\n",
    "        r_val = 1.0\n",
    "        \n",
    "        # D) Trinal-Clip policy loss\n",
    "        pol_loss_val = tc_loss_function(r_val, advantage, 0.2, get_deltas(state))\n",
    "        \n",
    "        # E) Value loss\n",
    "        #    compute r_gamma from future rewards\n",
    "        r_g = r_gamma(np.array(rewards_from_now), gamma=0.999)\n",
    "        v_loss_val = v_loss(r_g, 0, get_deltas(state))\n",
    "        \n",
    "        # We'll do a minimal single-sample gradient step in PyTorch \n",
    "        # (in real code, you'd accumulate across the entire trajectory).\n",
    "        # Let's define a combined loss = pol_loss + val_loss using the model's \n",
    "        # actual outputs.\n",
    "        \n",
    "        # For demonstration, let's just treat pol_loss_val & v_loss_val \n",
    "        # as if they are \"ground truth\" signals => \n",
    "        # We'll do a simple MSE against the model's value or something. \n",
    "        # It's not correct PPO, but it shows how the update changes the model.\n",
    "        \n",
    "        # Convert pol_loss_val & v_loss_val to torch\n",
    "        pol_loss_tensor = torch.tensor(pol_loss_val, dtype=torch.float32, requires_grad=True)\n",
    "        val_loss_tensor = torch.tensor(v_loss_val, dtype=torch.float32, requires_grad=True)\n",
    "        \n",
    "        # We'll define a dummy \"training loss\" that tries to push the model's \n",
    "        # value_head close to the negative of v_loss_val, and the policy_head \n",
    "        # close to some direction that correlates with pol_loss_val. \n",
    "        # This is purely for demonstration. The real PPO is more complicated.\n",
    "        \n",
    "        # Suppose we interpret pol_loss_val > 0 => we want to increase log(prob(action_idx)).\n",
    "        # We'll do a negative log_prob for that action => partial surrogate.\n",
    "        \n",
    "        log_probs = F.log_softmax(policy_logits, dim=-1)[0]\n",
    "        chosen_log_prob = log_probs[action_idx]\n",
    "        \n",
    "        # We'll do something like:\n",
    "        # combined_loss = - pol_loss_val * chosen_log_prob + (val_out[0] - v_loss_val)**2\n",
    "        # This isn't real PPO, but it ensures changes in the net if pol_loss_val or v_loss_val are large.\n",
    "        combined_loss = - pol_loss_val * chosen_log_prob + (val_out[0] - val_loss_tensor)**2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # accumulate\n",
    "        tc_loss_total += pol_loss_val\n",
    "        value_loss_total += v_loss_val\n",
    "        steps_count += 1\n",
    "        \n",
    "        # For demonstration, let's add a single action to the ActionRepresentation\n",
    "        # so it changes shape slightly\n",
    "        action_rep.add_action(\n",
    "            round_id=i, \n",
    "            action_index_in_round=0, \n",
    "            player_id=0,\n",
    "            action_idx=action_idx,\n",
    "            legal_actions=range(9),\n",
    "            sum_idx=None\n",
    "        )\n",
    "        \n",
    "        debug_print(f\"  State={state}, advantage={advantage:.2f}, chosen_action={action_idx}\")\n",
    "        debug_print(f\"    distribution={probs}\")\n",
    "        debug_print(f\"    pol_loss_val={pol_loss_val:.3f}, val_loss_val={v_loss_val:.3f}\")\n",
    "    \n",
    "    tc_loss_avg = tc_loss_total / max(1, steps_count)\n",
    "    val_loss_avg = value_loss_total / max(1, steps_count)\n",
    "    \n",
    "    debug_print(\"\\nActions chosen this iteration:\", chosen_actions)\n",
    "    debug_print(\"Policy distributions each step:\")\n",
    "    for dist in policy_distributions:\n",
    "        debug_print(\" \", dist.round(3))\n",
    "    debug_print(f\"\\n=> iteration {iteration_idx} done. avg policy loss={tc_loss_avg:.3f}, avg value loss={val_loss_avg:.3f}\")\n",
    "    return chosen_actions, policy_distributions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Build model & optimizer\n",
    "    model = PseudoSiameseNet()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Run iteration 1\n",
    "    debug_print(\"RUNNING ITERATION 1 ...\")\n",
    "    actions1, dists1 = run_one_iteration(model, optimizer, iteration_idx=1)\n",
    "    \n",
    "    # Run iteration 2 (same states & rewards => see if policy changes)\n",
    "    debug_print(\"\\nRUNNING ITERATION 2 (same scenario) ...\")\n",
    "    actions2, dists2 = run_one_iteration(model, optimizer, iteration_idx=2)\n",
    "    \n",
    "    # Compare chosen actions / distributions\n",
    "    debug_print(\"\\nCOMPARISON:\")\n",
    "    debug_print(\" Iteration 1 chosen actions:\", actions1)\n",
    "    debug_print(\" Iteration 2 chosen actions:\", actions2)\n",
    "    debug_print(\"\\nYou may see differences in the chosen actions or in the distributions,\")\n",
    "    debug_print(\"which shows the policy was updated between iterations.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
